[
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Jumpstart your cloud career with AWS SimuLearn by Denee McCloud, Hetvi Parsana, Karishma Damania, and Kattie Sepehri | on 25 JUN 2025 | in Amazon API Gateway, Amazon Bedrock, Amazon DynamoDB, Amazon SageMaker, AWS Config, AWS Lambda, Best Practices, Generative AI, Healthcare | Permalink | Share\nFor early-career cloud professionals, gaining hands-on experience with real customer engagements can be challenging. Even when shadowing opportunities exist, the fast-paced nature of these interactions rarely allows time to pause, reflect, and truly understand complex concepts.\nAWS SimuLearn addresses this gap by combining generative AI-driven customer simulations with practical technical training. Powered by Amazon Bedrock, AWS SimuLearn provides an immersive, risk-free environment where you can develop both technical and soft skills through interactive customer conversations, solution concept videos, hands-on labs, and practical exercises—all at your own pace.\nIn this post, we’ll follow three early-career AWS Solutions Architects who used these role-based learning plans and simulations to enhance their cloud expertise and customer-facing readiness. Their experiences demonstrate how structured practice in a controlled environment can accelerate professional development.\nHetvi’s story: Role-based learning focused on generative AI As a new Solutions Architect, I faced a common challenge: how to effectively communicate complex technical solutions to business leaders. While I was confident in my technical knowledge, translating cloud concepts into business value proved to be a different skill altogether.\nAWS SimuLearn transformed my approach to customer conversations. Through interactive simulations, I practiced engaging with virtual customers who needed generative AI solutions for their business challenges.\nOne memorable scenario involved helping a retail client understand how AI could enhance their customer experience through personalized shopping recommendations. I was provided with immediate feedback on both my technical recommendations and communication style. I learned to replace technical jargon with business outcomes, focusing on ROI and operational improvements rather than architectural specifications.\nWhat made the experience invaluable was the ability to retry conversations and experiment with different approaches. Each iteration helped me refine my message and build confidence in handling complex customer interactions.\nToday, I can confidently bridge the gap between technical solutions and business value, a skill that’s proven essential in my role as a Solutions Architect. For anyone looking to enhance their customer communication skills in technical roles, AWS SimuLearn offers a risk-free environment to practice and grow.\nKarishma’s story: Building technical depth As a Solutions Architect, I’ve found that deep technical expertise is essential for designing effective customer solutions. To expand my technical depth, I’ve combined individual training topics in AWS SimuLearn to create a structured learning experience tailored to real-world technical challenges.\nI started with security at the edge, progressing through modules on network security, encryption, identity and access management, and finally serverless technologies like Amazon API Gateway, AWS Lambda, and Amazon DynamoDB. Each module included simulated customer interactions where I practiced gathering requirements and proposing solutions, followed by hands-on labs to solidify my understanding.\nBy combining training modules, I was able to simulate customer-specific workflows, ensuring a deeper grasp of their challenges. This personalized approach didn’t just prepare me for customer presentations, it gave me a framework to continuously refine my technical depth. With AWS SimuLearn, I can now rapidly upskill in unfamiliar areas and confidently architect solutions that align with business objectives, making me a more effective Solutions Architect.\nKattie’s story: Industry-based learning My background is in designing and developing software tools for Health Care and Life Sciences (HCLS) research teams. While I’m familiar with several programming languages, I had limited cloud knowledge at the level of an AWS Certified Solutions Architect – Associate and wanted to expand my knowledge about how AWS can be utilized in the HCLS industry. Additionally, I wanted to gain in-depth knowledge about various AWS services, use cases, and integration with third-party software.\nThe AWS SimuLearn: Healthcare Learning Plan provided a wide range of business problems that gave me the opportunity to experiment with various AWS services to find technical solutions for the business problems. For example, I experimented with services relevant to HIPAA compliance with AWS Config and AWS Systems Manager and AI with Amazon SageMaker, as well as batch processing, analytics, database and storage, dashboards with real-time data processing, IoT, and more.\nEach learning plan can be started using the Open Dialogue mode or the Scripted mode. Initially, I chose the Scripted mode because it was less challenging, but once I observed a few different simulated conversations, I felt comfortable using the Open Dialogue mode. Open Dialogue mode offers an opportunity to hone your soft skills through an interactive, real-time customer conversation.\nAlong the way, AWS SimuLearn provides helpful hints to guide the conversation, as well as an incomplete architecture diagram for you to expand upon during the dialogue with the customer. You also get asked questions regarding different services to further enhance the learning experience.\nGet started with AWS SimuLearn Through simulated customer conversations and hands-on learning, AWS SimuLearn helped these three early-career professionals on their learning journeys. They received real-time feedback to improve their soft and technical skills while gaining experience in a live console environment.\nWith 200+ trainings, including learning options available by cloud role or industry, AWS SimuLearn offers you a personalized learning experience. Game-based training with simulations empowers early-career professionals to build both technical expertise and customer engagement skills in a risk-free environment.\nReady to launch your cloud career? Begin with the free AWS SimuLearn: Cloud Practitioner Learning Plan and explore the full library of topic-based simulations.\nTo learn more about what’s new with AWS SimuLearn, check out this recent blog post: Introducing AWS SimuLearn: Generative AI Practitioner.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "How to Set Up Automated Alerts for Newly Purchased AWS Savings Plans by Syed Muhammad Tawha and Dan Johns | on 26 JUN 2025 | in Amazon Simple Notification Service (SNS), AWS Cloud Financial Management, AWS CloudFormation, Cloud Cost Optimization | Permalink | Share\nAs organizations expand, FinOps teams require a comprehensive overview of AWS Savings Plans commitments to maximize utilization efficiency. This solution involves implementing monitoring systems and automated alerts to identify underutilized Savings Plans within the eligible return period.\nWhen you purchase a Savings Plan, you make a commitment for one or three years. Savings Plans with an hourly commitment of $100 or less can be returned if they were purchased within the last seven days and in the same calendar month, provided you haven’t reached your return limit. Once the calendar month ends (UTC time), these purchased Savings Plans cannot be returned.\nIn this blog post, we provide AWS CloudFormation templates that create AWS Step Functions state machine, Amazon Simple Notification Service (SNS) topic, Amazon EventBridge scheduler, and necessary AWS Identity and Access Management (IAM) roles to automate the monitoring of newly purchased Savings Plans and highlight those that are underutilized.\nOverview of Solution: This solution follows AWS security best practices by separating the deployment across two accounts. One CloudFormation stack will be created in the Management account to establish necessary IAM roles for fetching Savings Plans utilization. Another CloudFormation stack will be deployed in your chosen Member Account within your AWS Organization.\nThe CloudFormation stack in a member account creates a state machine that assumes a role in your management account and analyzes all Savings Plans in your management account, including those purchased across your organization. The workflow filters active Savings Plans based on their purchase date, focusing specifically on plans acquired within the last 7 days and the current calendar month. It then evaluates their utilization rates and identifies plans falling below the defined threshold.\nThe state machine executes at your specified frequency and uses Amazon SNS to send email alerts to addresses you provide during CloudFormation stack creation. These alerts contain detailed information about low-utilization Savings Plans and instructions for the return process.\nFigure 1: AWS architecture diagram – Member account assumes a role to read Savings Plans data from the management account and triggers a Step Function, which sends email alerts via SNS. Solution Walk Through: Prerequisites An AWS account IAM permissions to create a CloudFormation Stack and deploy an IAM role in the management Account IAM permissions to create a CloudFormation Stack and deploy Step Functions, IAM roles, SNS, and EventBridge scheduler in your chosen member Account Deploy the solution In this section we will deploy resources for this solution in your accounts:\nPart 1 – Member Account Deployment In this section, we will deploy resources for this solution in your chosen member account.\nLogin to your AWS Management Console of the member account where you want this solution to run Deploy this CloudFormation Stack Provide the Stack Name as new-sp-utilization-alert-member In the AlertEmails parameter, enter a comma-separated list of email addresses that will receive notifications about underutilized Savings Plans. In the ManagementAccountId parameter, enter the 12 digit AWS Account Id of your AWS management account. In the ScheduleExpression parameter, specify the execution frequency for the Step Functions state machine using cron format (default is daily at 9 AM UTC). In the UtilizationThreshold parameter, specify the minimum utilization percentage for your Savings Plans. You receive alerts when utilization falls below this threshold. Click Next, select the acknowledgment box, and create the stack Wait until the stack has finished deploying and is showing as CREATE-COMPLETE You will receive an email to confirm your subscription to the SNS topic created by this stack. Please confirm the subscription to begin receiving notifications. Visit the Outputs tab of the stack you just created and make a note of the values of the ExecutionRoleArn and StateMachineArn Keys, you will need these in the next part. Part 2 – Management Account Deployment Log in to your AWS Management Console. Note: This must be the same account as the one entered in the ManagementAccountId parameter in the previous part. Deploy this CloudFormation stack Provide the Stack Name as new-sp-utilization-alert-management In the ExecutionRoleArn parameter, provide the value copied from the stack outputs of the stack deployed in the member account. In the StateMachineArn parameter, provide the value copied from the stack outputs of the stack deployed in the member account. Click Next, select the acknowledgment box, and create the stack Wait until the stack has finished deploying and is showing as CREATE-COMPLETE Test the Solution Now that the Step Functions state machine and associated resources are deployed in your member account, let’s test the deployment:\nLogin back in to your AWS Management Console of the member account where you deployed part 1 of this solution. Navigate to the Resources tab in your CloudFormation stack and locate the SavingsPlansAlerts Step Functions state machine. Click the blue hyperlink. You will be redirected to the Step Functions console. Click the Start execution button on the right. View the execution details in the Events section to monitor the state machine’s progress. If you have any Savings Plans purchased within the last 7 days and the current calendar month, you will receive email notifications. A successful execution is indicated by a green box in the Graph view. If any Savings Plans fall below your specified utilization threshold, you will receive an email at your provided address. Clean Up All resources deployed for this solution can be removed by deleting the CloudFormation stacks. You can delete the stack through either the AWS Management Console or the AWS CLI.\nTo delete the management account stack (CLI):\naws cloudformation delete-stack –stack-name new-sp-utilization-alert_management To delete the member account stack (CLI):\naws cloudformation delete-stack –stack-name new-sp-utilization-alert_member Understanding Alerts and Taking Action When you receive an alert about underutilized Savings Plans, you should review the utilization details provided in the email notification. Analyze your utilization metrics against the original commitment you made when purchasing the Savings Plan, and investigate whether the low utilization is an expected or due to other factors such as workload migration, architectural changes, or miscalculated capacity needs. Consider returning the Savings Plan if the utilization remains consistently below your threshold, the plan was purchased within the last 7 days, the purchase occurred in the current calendar month, and the hourly commitment is $100 or less. Document the return reason for future reference and planning.\nConclusion In this post, we explored how to use the Savings Plan and Cost Explorer APIs to identify underutilized Savings Plans in your organization. We then demonstrated how to use a Step Functions State Machine to filter Savings Plans purchased within the last 7 days and the current calendar month. This timing is crucial because you can return Savings Plans within the return window if they were purchased inadvertently or aren’t being utilized effectively. For guidance on returning a purchased Savings Plan, please refer to the Returning a Purchased Savings Plan documentation.\nSyed Muhammad Tawha\nSyed Muhammad Tawha is a Principal Technical Account Manager at AWS based in Dublin, Ireland. Tawha specializes in Storage, Resilience and Cloud Cost Optimization. He is passionate about helping AWS customers. Tawha also loves spending time with his friends and family. Dan Johns\nDan Johns is a Senior Solutions Architect Engineer, supporting his customers to build on AWS and deliver on business requirements. Away from professional life, he loves reading, spending time with his family and automating tasks within their home. "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Operating BYOL Windows Server Workloads Effectively on AWS by Ali Alzand, Jon Madison, and Mike Gupta | on 01 JUL 2025 | in Amazon EC2, AWS Config, AWS Cost and Usage Report, AWS License Manager, AWS Migration Hub, Technical How-to, Windows on AWS | Permalink | Share\nOne way that customers running Microsoft Workloads on Amazon Web Services (AWS) may reduce costs is taking advantage of Bring Your Own License (BYOL) for eligible licenses they own. In this blog post, we are going to share a few practices to help you optimize your operation of BYOL Windows Server workloads on AWS.\nIntroduction A common way to run your Windows Server workloads on Amazon Elastic Compute Cloud (Amazon EC2) is to use the “license included” option. This has the benefit of not having to purchase or manage your own licenses and the flexibility of per-second billing. However, if you have already purchased licenses and they are eligible for use on AWS, then it makes sense to bring them and reduce your costs accordingly.\nWe will review several specific techniques to help you when running BYOL Windows Server workloads on AWS. They are:\nPreparing your on-premises servers for import to AWS as Amazon Machine Images (AMIs). Transition and manage your Windows licenses from BYOL to license included when appropriate. Detecting configuration issues using an AWS Config custom rule. Understanding data related to your BYOL Windows instances in the AWS Cost and Usage Report (AWS CUR). BYOL for Windows on AWS To take advantage of BYOL, you need to confirm that they are eligible. AWS provides guidance for Microsoft Licensing on AWS. When determining if your Windows licenses are eligible for BYOL on AWS, consider:\nLicenses must be perpetual, and purchased before October 1, 2019, or as a true-up on an Enterprise Agreement (EA) that was active at that time. The Windows version must be Windows Server 2019 or earlier. If your licenses are eligible, then you can use them on AWS. Regardless of whether or not you have Software Assurance on your licenses, Windows Server is not eligible for License Mobility. This means that the licenses will need to apply to hardware dedicated to you alone. Amazon EC2 Dedicated Hosts are a solution that fulfills this requirement. Dedicated Hosts provide you with a familiar experience for running your Amazon EC2 instances, without the need to manage hardware or a hypervisor. AWS License Manager is a service used to manage licenses in AWS, and it is key to an effective BYOL Windows strategy.\nThe billing for your Amazon EC2 Windows instances is determined from the usage operation field that the instance inherits from its source AMI. Windows instances that run with the license included, regardless of tenancy, use the usage operation of RunInstances:0002. However, when you use your own license for a Windows instance on dedicated hosts, the usage operation of RunInstances:0800 is required. The how to create an Amazon EC2 AMI usage and billing information report blog post will help you generate the usage operation for the instances in your organization.\nPreparing your images for BYOL One requirement for using your own Windows licenses on AWS is to supply your own AMI, rather than using one created by AWS. When bringing your own image to AWS, you have different options to produce them. If the destination for your Windows server is BYOL on dedicated hosts, these tools will help you ensure your AMI is ready for use.\nVM Import/Export (VMIE) is a tool that helps you to import virtual machine images from your existing virtualization platform as Amazon Machine Images. The first step is to export your virtual machine using a standard format such as Open Virtual Appliance (OVA), ESX Virtual Machine Disk (VMDK), or Virtual Hard Disk (VHD/VHDX). Then, upload the image to an Amazon Simple Storage Service (S3) bucket in anticipation of the conversion process.\nTo use VMIE, use these instructions to create an AWS Identity and Access Management (IAM) role named “vmimport” that the service will use to perform operations on your behalf.\nWhen using the AWS Command Line Interface (AWS CLI) to import a Windows image that you are planning on using for BYOL on dedicated hosts, it is necessary to specify the license type to set the usage operation correctly on the resultant AMI. To import an image, a command such as the following can be used (in this case for an OVA image in an S3 bucket):\naws ec2 import-image –usage-operation RunInstances:0800 –disk-containers Format=OVA,Url=s3://\u0026lt;\u0026lt;my-bucket\u0026gt;\u0026gt;/\u0026lt;\u0026lt;my-image-name\u0026gt;\u0026gt;.ova This will start an import job that, once completed, will yield an AMI with the proper usage code for Windows BYOL.\nMigration Hub Orchestrator is a tool that lets you create workflows to automate tasks and simplify the migration process. One of the workflow templates that Orchestrator provides is “Import virtual machine images to AWS”. Use this workflow to import an image for Windows BYOL.\nOpen the AWS console and navigate to the Migration Hub Console. Choose Workflows in the Orchestrate side menu. Choose Create Workflow (Figure 1) Figure 1: Create Workflow Select the Import virtual machine images to AWS template (Figure 2) and choose Next Figure 2: Select the import virtual machine template On the Configure your workflow page, enter a Name for the workflow, and optionally enter a Description. In the Source environment configuration section, populate the Disk container field, which is the S3 bucket where you stored your image from on premises. The name must conform to the requirements from the Migration Hub Orchestrator documentation. Figure 3: Configure source environment In the Target environment configuration section, select the operating system and license for the virtual machines created with the resultant AMI. Choose Windows Server BYOL without SQL Server. Figure 4: Choose the licensing model Use the rest of the fields to further customize your AMI based on your requirements. These include the boot mode, AWS Key Management Service (KMS) encryption key, tags and license specification (for business case analysis). You also have the option to leave these with their default values. Choose On the Review and submit page, choose After uploading an image and creating your workflow, it is ready to run by choosing Run workflow.\nFigure 5: Run Workflow Managing license conversion properly There are scenarios in which you will need to switch Amazon EC2 instances from the BYOL licensing model to license included and vice versa. These include (but are not limited to):\nUpgrading the operating system of the Amazon EC2 instance to Windows Server 2022, which is not eligible for BYOL, regardless of tenancy. Moving an Amazon EC2 instance off a dedicated host to run it on shared tenancy EC2, which is not eligible for BYOL. Moving an Amazon EC2 instance that is eligible for BYOL from shared tenancy to a dedicated host. When you need to switch the licensing model of an Amazon EC2 instance, use the License type conversion feature in AWS License Manager. License type conversion lets you change the usage operation. See our guide for eligible license types for Windows and SQL Server in License Manager.\nDetecting configuration issues with AWS Config AWS Config is a service that helps you assess, audit, and evaluate the configuration of your AWS resources. By leveraging a custom AWS Config Rule, you can detect potential license misconfiguration in instances running on dedicated hosts, saving unnecessary licensing costs.\nThe aws-config-rules repository contains custom AWS Config Rules to deploy to your AWS account using the AWS Config Rules Development Kit (RDK). Use the custom AWS Config Rule called EC2_INSTANCE_LICENSE_INCLUDED_DEDICATED_HOST to detect instances with license-included Windows Server (usage operation RunInstances:0002) running on Dedicated Hosts.\nUse AWS CloudShell to run the RDK and test the AWS Config rules deployment. To install the custom rule, open CloudShell in the AWS Console in the desired AWS Region, and run the following commands:\npip install rdk rdk init git clone https://github.com/awslabs/aws-config-rules cd aws-config-rules/python rdk deploy EC2_INSTANCE_LICENSE_INCLUDED_DEDICATED_HOST Once the rule has completed deployment, view the rule in the AWS Config console. For instances with mis-configured licenses, either move them to Shared tenancy or follow the License Conversion process accordingly.\nFigure 6: Custom Config rule Understanding CUR data for BYOL instances AWS Cost and Usage Reports (CUR) contains the most comprehensive set of cost and usage data available. Use Amazon Athena to query your CUR data. The following query shows the licenses your instances are being billed for:\nselect line_item_resource_id, line_item_operation, line_item_line_item_type, month, year, line_item_unblended_cost, line_item_blended_cost, line_item_usage_type, line_item_usage_account_id, line_item_line_item_description from customer_all where line_item_usage_account_id = \u0026#39;[ACCOUNT NUMBER]\u0026#39; and line_item_line_item_type = \u0026#39;Usage\u0026#39; and line_item_operation like \u0026#39;%RunInstances:%\u0026#39; Based on the results of the above Query, the line_item_operation field shows what you’re being billed for.\nFigure 7: AWS CUR output Conclusion Implementing BYOL for Windows Server workloads on AWS successfully, requires careful attention to license eligibility, configuration, and ongoing management. By understanding the key requirements – from license purchase dates and Windows Server versions to proper usage operation codes on dedicated hosts – organizations can effectively reduce their cloud infrastructure costs while maintaining compliance. Success depends on three key elements:\nProper license evaluation – identifying eligible licenses based on purchase date and Windows Server version Accurate configuration – ensuring correct usage operation codes to avoid double-billing on dedicated hosts Ongoing monitoring – maintaining regular assessment of usage and costs By following these practices, organizations can optimize their Windows Server deployment costs while maintaining licensing compliance on AWS.\nReady to start optimizing your Windows Server costs on AWS? Request an AWS Optimization and Licensing Assessment to begin evaluating your licensing opportunities and potential cost savings.\nAWS has significantly more services, and more features within those services, than any other cloud provider, making it faster, easier, and more cost effective to move your existing applications to the cloud and build nearly anything you can imagine. Give your Microsoft applications the infrastructure they need to drive the business outcomes you want. Visit our .NET on AWS and AWS Database blogs for additional guidance and options for your Microsoft workloads. Contact us to start your migration and modernization journey today.\nTAGS: Amazon EC2, AWS License Manager, Cost Savings, Microsoft, Windows On AWS, Windows Server\nAli Alzand\nAli is a Microsoft Specialist Solutions Architect at Amazon Web Services who helps global customers unlock the power of the cloud by migrating, modernizing, and optimizing their Microsoft workloads. He specializes in cloud operations — leveraging AWS services like Systems Manager, Amazon EC2 Windows, and EC2 Image Builder to drive cloud transformation. Outside of work, Ali enjoys exploring the outdoors, firing up the grill on weekends for barbecue with friends, and sampling all the eclectic food has to offer. Jon Madison\nJon Madison is a Pr. Delivery Consultant on the AWS Professional Services (ProServe) Energy Team. He has a background in Cloud Infrastructure, Security, and DevOps, and is passionate about helping customers with cloud adoption and building scalable solutions and processes. In his free time Jon enjoys cooking, gaming, and spending time with his family and friends. Mike Gupta\nMike Gupta is a Senior Technical Account Manager at AWS based out of New York City. In his role, he provides strategic technical guidance to help customers use AWS best practices to plan and build solutions. He’s dedicated to empower customers to develop scalable, resilient, and cost-effective architectures. In his free time, Mike enjoys spending time with his wife and family, exploring local history and trying new restaurants. "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Summary Report: “AI-Driven Development Life Cycle: Reimagining Software Engineering” Event Objectives Explores AI-Driven Development using Amazon Q Developer Explores Spec-Driven Development using Kiro Speakers Toan Huynh – SA My Nguyen – ISV Account Director Key Highlights The Role of AI in IT 2023: AI acted as an assistant, helping developers minimize workload, recommend code, and check errors 2024: AI became embedded in IDEs, enabling system design and automated workflows 2025: Software engineers with strong AI skills can develop products in minimal time Developing Products with AI While Keeping Humans at the Center Developers can use AI to generate code but they still must validate its correctness and effectiveness The owner of AI-generated code is the developer, since they are responsible for validating it before implementation Coding with AI usually leads to two paths: AI-Assistant: Developers see AI as a co-worker who suggests code and solutions AI-Manager: AI takes full responsibility for designing systems, generating and implementing code, and maintaining the product AI-Driven Development Lifecycle (AI-DLC) AI acts as an assistant that helps to develop the requirements and specifications AI supports developers by writing code in small units step-by-step, waiting for validation before moving on to the next task It requires developers to minimize the scope and provide a specified prompt for AI Mob programming is recommended for this methodology since it reduces product development time Spec-Driven Development Lifecycle with Kiro Kiro is an IDE that supports developers by taking specifications and developing step-by-step tasks to implement them into code Kiro provides a log of all the actions and prompts, allowing developers to track progress Kiro is suitable for projects that need to have accurate specifications before starting to write code Key Takeaways Developer Mindset Human-centric: AI can design systems and write code, but developers remain responsible for validating the results Ready for changes: With powerful AI tools like Amazon Q Developer and Kiro emerging, developers should continuously prepare for new technologies Pros and Cons Using Ai-Driven or Spec-Driven Development Lifecycle depends on the scope and purpose of the project Each method has pros and cons that affect the development team as well as the product Pros AI-Driven Development Lifecycle is good for projects that need the speed, reusability, cost efficiency, and scalability Spec-Driven Development Lifecycle is good for projects that need narrative quality, consistency, and predictability Cons AI-Driven Development Lifecycle may result in inconsistent quality, ethical/IP concerns, or maintenance challenges Spec-Driven Development Lifecycle may result in slower production cycles, higher costs, or limited content Applying to Work Apply AI-DLC or SDD to current projects Learn new technologies like Amazon Q Developer or Kiro Event Experience Attending the “AI-Driven Development Life Cycle: Reimagining Software Engineering” workshop was extremely valuable. I had the opportunity to learn new methodologies and connect with IT professionals. Key experiences included:\nLearning from highly skilled speakers Learning from experts how they handle errors in coding and product development Experiencing real-world use cases of the methodologies Leveraging modern tools Explored Amazon Q Developer, an AI tool that supports AI-DLC Observed a demonstration of how Kiro is used in the Spec-Driven Development Lifecycle Overall, the event not only provided technical knowledge but also introduced to new tools that can support development lifecycle\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Tran Ngoc Khiet\nPhone Number: 0964171773\nEmail: tranngockhiet22062005@gmail.com\nUniversity: FPT University Ho Chi Minh Campus\nMajor: Information Technology\nClass: AWS\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FACJ Cloud Intern\nInternship Duration: From 09/06/2025 to 12/24/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Serverless \u0026amp; Event-Driven Architecture Serverless Architecture: This workshop utilizes a cloud-native model with services like AWS Lambda, Amazon API Gateway, and Amazon DynamoDB. This approach allows code to run in response to requests without provisioning or managing servers, as AWS handles all automatic scaling and infrastructure management. Event-Driven Architecture: The core of the system functions on an event-driven basis. Instead of services continuously polling for data, specific events—such as IoT sensor readings or user API calls—trigger downstream workflows. This is orchestrated by AWS IoT Core and Amazon EventBridge, creating a highly flexible and scalable system. Workshop overview In this workshop, you will deploy a comprehensive serverless data platform on AWS to manage real-time environmental monitoring for an 8-room smart office setup. The system integrates AWS IoT Core, Lambda, DynamoDB, S3, CloudFront, and Amazon Cognito. Sensor data is forwarded from edge devices (or simulated scripts), ingested into AWS, stored in DynamoDB tables, and processed by Lambda functions to update the management dashboard. Critical events are routed through EventBridge for alerting, demonstrating a high-availability, low-cost, and seamless scalability architecture.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Team up for the project, get acquainted with FCJ mentor team. Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube from the begining to the end of module 2. Learn about AWS services and how to use them. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Team up and get acquainted with team members - Read rules, workshop guide and handling of violations - Watch video How to draw AWS architecture with draw.io - Watch video Guide for AWS workshop 09/08/2025 09/08/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://policies.fcjuni.com 3 - Watch theory videos of Module 1 - Watch videos, read documents of Lab 1, Lab 7 and Lab 9 - Practice: + Create AWS Free Tier Account + Config MFA for the account + Upgrade account to paid plan 09/09/2025 09/09/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000001.awsstudygroup.com https://000007.awsstudygroup.com https://000009.awsstudygroup.com 4 - Watch theory videos of Module 2 - Watch videos, read documents of Lab 3 - Practice: + Create VPC + Create Subnet + Create Route Table + Create Internet Gateway + Create NAT Gateway + Create Security Group + Create Network ACL + Create EC2 instances + Test connection between two EC2 Instances 09/10/2025 09/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000003.awsstudygroup.com 5 - Watch videos, read documents of Lab 10 and Lab 19 - Practice: + Create and test a connection to a Hybrid DNS with Route 53 + Create and test a VPC Peering connection between two EC2 instances 09/11/2025 09/11/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000010.awsstudygroup.com https://000019.awsstudygroup.com 6 - Watch videos, read documents of Lab 20 - Practice: + Create a Transit Gateway + Test the connection between EC2 instances through the Transit Gateway 09/12/2025 09/12/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000020.awsstudygroup.com Week 1 Achievements: Create AWS Free Tier account, upgrade to paid plan and config MFA sucessfully. Get used to AWS Management Console and add widgets to the screen. Create and config AWS services: VPC Subnet Route Table Internet Gateway NAT Gateway Security Group Network ACL EC2 Know how to create CloudFormation from templates. Know how to connect EC2 Intances through VPC Peering Connection or Transit Gateway. Know how to create and connect to DNS with Route 53. "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube from Module 3 to the end of module 4. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory videos of Module 3 - Watch videos, read documents of Lab 13 and Lab 24 - Practice: + Create and test the operation of the Backup Plan + Create and connect Storage Gateway to on-premise environment through file share 09/15/2025 09/15/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000013.awsstudygroup.com https://000024.awsstudygroup.com 3 - Watch videos, read documents of Lab 57 - Practice: + Create and config S3 Bucket + Host a static website with S3 and through CloudFront + Config bucket versioning, move object among Bucket and replicate object among multi-region Bucket 09/16/2025 09/16/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000057.awsstudygroup.com 4 - Watch theory videos of Module 4 - Watch videos, read documents of Lab 14 - Practice: + Import a Virtual Machine to AWS + Deploy an Instance from AMI + Export a Virtual Machine from an Instance 09/17/2025 09/17/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000014.awsstudygroup.com 5 - Watch videos, read documents of Lab 25 - Practice: + Create an SSD/HDD Multi-AZ file system and config File Share + Operate some functions of File Share through Remote Desktop 09/18/2025 09/18/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000025.awsstudygroup.com 6 - Learn Coursera course Research Methods and Research Methodologies 09/19/2025 09/19/2025 https://www.coursera.org/learn/research-methods https://www.coursera.org/learn/research-methodologies Week 2 Achievements: Know how to create Backup Plan Know how to create Storage Gateway and config File Share Know how to create and config S3 Bucket Know how to host a static website with S3 and through CloudFront Practice some of S3 Bucket functions: Bucket versioning Move object Replicate object Know how to import/export VM Know how to deploy an Instance from AMI Know how to operate some functions on File Share of an SSD/HDD Multi-AZ file system through CLI on a Remote Desktop "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube Module 5. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory videos of Module 5 - Watch videos, read documents of Lab 18 and Lab 22 - Practice: + Config Security Hub + Create and test Lambda Function 09/22/2025 09/22/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000018.awsstudygroup.com https://000022.awsstudygroup.com 3 - Watch videos, read documents of Lab 27 and Lab 28 - Practice: + Create, manage Tag and Resource Group + Config Policies for Role + Test access EC2 with Tag through IAM 09/23/2025 09/23/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000027.awsstudygroup.com https://000028.awsstudygroup.com 4 - Watch videos, read documents of Lab 30 and Lab 33 - Practice: + Upload and share encrypted data with S3 Bucket + Create and query Event History with Athena through CloudTrail table 09/24/2025 08/24/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000030.awsstudygroup.com https://000033.awsstudygroup.com 5 - Watch videos, read documents of Lab 44 and Lab 48 - Practice: + Config IAM Group, User and Role + Config Access Key and use IAM Role to access 09/25/2025 08/25/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000044.awsstudygroup.com https://000048.awsstudygroup.com 6 - Learn Coursera course Being a researcher (in Information Science and Technology) và Advanced Writing 09/26/2025 08/26/2025 https://www.coursera.org/learn/being-researcher https://www.coursera.org/learn/advanced-writing?specialization=academic-english Week 3 Achievements: Know how to enable and config Security Hub Know how to create and use Lambda Function Know how to mange resource with Tag and Resource Group Know how to attach Policies to Role Know how to share encrypted data through S3 Bucket Know how to query Event History with Athena through Cloudtrail table Know how to mange User through Group, Role and Access key "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube Module 6. Participate in AI-Driven Development Life Cycle: Reimagining Software Engineering event. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory videos of Module 6 09/29/2025 09/29/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 3 - Watch videos, read documents of Lab 5 - Practice: + Create RDS Database Instance + Deploy application with NodeJS 09/30/2025 09/30/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000005.awsstudygroup.com 4 - Watch videos, read documents of Lab 35 - Practice: + Create sample data and Data Stream + Analysis and visualize the Data with Athena and QuickSight 10/01/2025 10/01/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000035.awsstudygroup.com 5 - Translate Blogs: + Jumpstart your cloud career with AWS SimuLearn + Observing Agentic AI workloads using Amazon CloudWatch + Operating BYOL Windows Server Workloads Effectively on AWS 10/02/2025 10/02/2025 https://aws.amazon.com/blogs/training-and-certification/jumpstart-your-cloud-career-with-aws-simulearn/ https://aws.amazon.com/blogs/aws-cloud-financial-management/how-to-set-up-automated-alerts-for-newly-purchased-aws-savings-plans/ https://aws.amazon.com/blogs/modernizing-with-aws/operating-byol-windows-server-workloads-effectively-on-aws/ 6 - Participate in AI-Driven Development Life Cycle: Reimagining Software Engineering event - Write event report 10/03/2025 10/03/2025 Week 4 Achievements: Know how to create RDS Database Instance and deploy application through it Know how to create Data Stream, analysis and visualize data through Athena and QuickSight Translate three blogs to Vietnamese Participate in AI-Driven Development Life Cycle: Reimagining Software Engineering event: Learn about AI-Driven Deployment Lifecycle through Amazon Q Developer Learn about Spec-Driven Deployment Lifecycle through Kiro "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Watch videos from series First Cloud Journey Bootcamp - 2025 in Youtube Module 7. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch videos, read documents of Lab 39 - Practice: + Create DyanamoDB database + Operate some query of DynamoDB in AWS Cloudshell 10/06/2025 10/06/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000039.awsstudygroup.com/vi 3 - Watch videos, read documents of Lab 40 - Practice: + Build database from S3 Bucket and Glue Crawler + Analysis cost and usage performance 10/07/2025 10/07/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000040.awsstudygroup.com/vi 4 - Watch videos, read documents of Lab 60 - Practice: + Create DyanamoDB database + Operate some DynamoDB functions on management console and AWS SDK: * Create * Write * Read * Update * Query * Create Global Secondary Index * Query Global Secondary Index 10/08/2025 10/08/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000060.awsstudygroup.com/vi 5 - Watch videos, read documents of Lab 72 - Practice: + Create Firehose, IAM Role, Glue Crawler + Transform data with notebook and Glue DataBrew + Analysis data with Athena, Kinesis Data Analytics + Visualize data with QuickSight + Run Lambda Function on data + Create and connect Redshift cluster with S3 Bucket through Glue Connection 10/09/2025 10/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000072.awsstudygroup.com/vi 6 - Watch videos, read documents of Lab 73 - Practice: + Upload dataset + Create visual chart and table from data + Create backup dashboard + Publish dashboard 10/10/2025 10/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000073.awsstudygroup.com/vi Week 5 Achievements: Know how to create DynamoDB database know how to query data using AWS CloudShell, AWS SDK or management console Know how to build database from S3 Bucket and Glue Crawler Know how to analysis cost and usage performance of Glue database using Athena Apply knowlegde of Kinesis, IAM, Glue, Athena, QuickSight, Lambda, Redshift to do a workshop Know how to create visual chart and table from data with Quicksight Know how to create and publish dashboard with QuickSight "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Synthesize knowledge about AWS to mindmap for mid-term test Brainstorm idea for workshop Draw architect diagram for workshop Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Synthesize theory knowledge of Module 1 and Module 2 - Draw mindmap for Module 1 and Module 2 10/13/2025 10/13/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://drive.google.com/file/d/1jlnUWvbaxQmmhaDT4eMMyFaZNcN3qlxt/view?usp=drive_link 3 - Synthesize theory knowledge of Module 3 and Module 4 - Draw mindmap for Module 3 and Module 4 10/14/2025 10/14/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://drive.google.com/file/d/1jlnUWvbaxQmmhaDT4eMMyFaZNcN3qlxt/view?usp=drive_link 4 - Synthesize theory knowledge of Module 5 and Module 6 - Draw mindmap for Module 5 and Module 6 10/15/2025 10/15/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://drive.google.com/file/d/1jlnUWvbaxQmmhaDT4eMMyFaZNcN3qlxt/view?usp=drive_link 5 - Brainstorm idea for workshop 10/16/2025 10/16/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 6 - Draw architect diagram for workshop 10/17/2025 10/17/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing Week 6 Achievements: Have a mindmap to summarize knowledge of AWS Brainstorm idea for workshop Draw architect diagram of workshop "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Caculate pricing and estimate Adjust architect diagram Write proposal Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Estimate and caculate price for: emsp; - Amazon DynamoDB emsp; - AWS Lambda emsp; - AWS iot core 10/20/2025 10/21/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 3 - Estimate and caculate price for: emsp; - Amazon API Gateway emsp; - AWS Simple Cloud Storage emsp; - Amazon CloudFront 10/21/2025 10/22/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 4 - Estimate and caculate price for: emsp; - Amazon EventBridge emsp; - Amazon SQS emsp; - Amazon CloudWatch 10/22/2025 10/23/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 5 - Estimate and caculate price for: emsp; - Amazon Simple Notification Service emsp; - Amazon Cognito emsp; - Create Estimate for workshop 10/23/2025 10/24/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing 6 - Adjust architect diagram and write proposal 10/24/2025 10/25/2025 https://drive.google.com/file/d/1QV5zNEUsXdX9vLxQRzBingVShyj8AHY0/view?usp=sharing Week 7 Achievements: Finish pricing and estimate Finish architect diagram Finish proposal "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Doing workshop for Amazon DynamoDB and Amazon Cognito Review knowledge for mid-term test Taking mid-term test Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Doing workshop for Amazon DynamoDB 10/27/2025 10/27/2025 3 - Doing workshop for Amazon Cognito 10/28/2025 10/28/2025 4 - Review mid-term test content 10/29/2025 10/29/2025 5 - Doing quiz created by NotebookLM for mid-term test 10/30/2025 10/30/2025 6 - Take mid-term test 10/31/2025 10/31/2025 Week 8 Achievements: Finish configuring Amazon DynamoDB for workshop Finish configuring Amazon Cognito for workshop Finish mid-term test "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Learn Python and Linux Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Module 1 and Module 2 11/03/2025 11/03/2025 https://www.udemy.com/course/python-mastery-the-complete-web-programming-course/?couponCode=KEEPLEARNING 3 - Learn Module 3 and Module 4 11/04/2025 11/04/2025 https://www.udemy.com/course/python-mastery-the-complete-web-programming-course/?couponCode=KEEPLEARNING 4 - Learn Module 5 and Module 6 11/05/2025 11/05/2025 https://www.udemy.com/course/python-mastery-the-complete-web-programming-course/?couponCode=KEEPLEARNING 5 - Learn Module 7 and Module 8 11/06/2025 11/06/2025 https://www.udemy.com/course/python-mastery-the-complete-web-programming-course/?couponCode=KEEPLEARNING 6 - Install and learn some basic code syntaxes of Linux Ubuntu on Virutal Box 11/07/2025 11/07/2025 https://www.youtube.com/watch?v=lmeDvSgN6zY Week 9 Achievements: Know some codde syntax of Python Know some codde syntax of Bash in Linux Ubuntu Summarize knowledge to note website https://programming-note-fawn.vercel.app/ "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Team up for project, learn Module 1 and 2\nWeek 2: Learn Module 3 and 4\nWeek 3: Learn Module 5\nWeek 4: Learn Module 6 and participate in an event\nWeek 5: Learn Module 7\nWeek 6: Create a mindmap to summarize knowleadge, brainstorm idea for workshop and draw architect diagram\nWeek 7: Caculate pricing and adjust architect diagram\nWeek 8: Doing workshop for DynamoDb and Cognito\nWeek 9: Learn Python and Linux\nWeek 10: Learn to deploy website from Gitlab and write Lambda Functions for website\nWeek 11: Learn JavaScript, TypeScript and expand some website functions\nWeek 12: Set up CloudFormation templates for workshop\nWeek 13: Debug, update API and write workshop detail\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\u0026rdquo; Event Objectives Introduce to Foundation Model ​Prompting techniques guide Explore Generative AI with Amazon Bedrock ​Retrieval-Augmented Generation (RAG): Architecture \u0026amp; Knowledge Base integration Speakers Danh Hoang Hieu Nghi Lam Truong Kiet Dinh Le Hoang Anh Key Highlights Prompting Techniques Effective prompting techniques for better and accurate result RAG (Retrieval-Augmented Generation) AI with embedded data source to response with more accurate, real-time and included internal data result Embedding Numerical representation of text (vectors) that captures semantics and relationships betwwen words Embedding models capture features and nuances of the text Rich embedding models can be used to compare text similarity Multilingual Text Embeddings can identify mearing in different languages Some AWS AI Service Amazon Rekognition Amazon Translate Amazon Textract Amazon Transcribe Amazon Polly Amazon Comprehend Amazon Kendra Amazon Lookout Aamzon Personalize Key Takeaways Prompting Techniques - Chain of thought Providing AI with an example case Explain for AI step-by-step to handle your problem RAG use cases Reducing hallucinations and connecting with recent knowledge including enterprise data Enchance chatbox capabilities by integrating with real-time data Searching base on user previous search history and persona Retrieving and summarizing transactional data from data Amazon Titan Embedding Translate text inputs (words, phrases) into numerical representations (embeddings) Comparing embeddings produces more relevant and contextual responses than word matching Max Tokens: 8000 Output Vectors: 256,512,1024 Language: Multilingual (100+ languages in preview) Applying to Work Explore more AI Services to apply for future project Event Experience Attending the “AI/ML/GenAI on AWS” workshop was extremely valuable. I had the opportunity to learn new knowlegde and connect with IT professionals. Key experiences included:\nLearning from highly skilled speakers Experts from FACJ shared best practices in applying AI in real project Through real-world case studies, I gained a deeper understanding of applying prompting techniques Explores AWS AI Service Through the demo of speakers, I learn how AWS AI service work and their real life use case Building agents guide Receiving experience and knowlege when start to build AI Agent Some event photos Overall, the event have give me a lot of knowlegde about AI to apply for real life project.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "Create IAM User for this workshop In AWS Management Console, search and choose IAM Navigate to User, click Create user For User name, enter admin-user Check Provide user access to the AWS Management Console - optional For Console password, check Custom password Enter password for your user Uncheck Users must create a new password at next sign-in - Recommended for easy operation Click Next For Permissions options, check Attach policies directly Click Create policy You will be directed to Create policy page For Policy editor, switch to JSON Copy and paste this policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;InfrastructureManagement\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;iam:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;BackendComputeAndAPI\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:*\u0026#34;, \u0026#34;apigateway:*\u0026#34;, \u0026#34;execute-api:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DatabaseAndAuth\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:*\u0026#34;, \u0026#34;cognito-idp:*\u0026#34;, \u0026#34;cognito-identity:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;IoTServices\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iot:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;StorageAndHosting\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:*\u0026#34;, \u0026#34;cloudfront:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;MonitoringAndLogging\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;events:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Click Next For Policy name, enter your policy name (E.g. SmartOfficeAdminFullAcccess) Add Tags for cost and operation management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) Click Create policy Go back to Step 2 Set permissions of Create user Search and choose your policy name Click Next Add Tags for cost and operation management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) Click Create user Login with your User account to begin this workhop "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "IoT Weather Platform for Lab Research A Unified AWS Serverless Solution for Real-Time Weather Monitoring 1. Executive Summary The Smart Office Management Console is designed base on the idea of weekly trip to AWS office at Ho Chi Minh city. It supports office with smart device like auto light, auto scensor window, auto air conditioner by collect, transmit and process data from IoT Sensor. By utilizing AWS Serverless services, data can transmit via MQTT and process every 5 minutes to support real-time monitoring, acting base on predictive weather and saving cost from unused devicesm with restricted to athenticated members via Amazon Cognito\n2. Problem Statement What’s the Problem? Nowadays, offices require manual control for light, window, air conditioner, .etc. Most office turn on light and air conditioner thoughout there work hour (usually 8 a.m to 5 p.m), but this may not be necessary. For example, sunlight at 8 a.m may not be bright enough for worker to see clearly, but if the office is not very big, sunlight from 10 a.m to 3.pm can support light up very much. Additionally, in rainy season, some places have high humidity which can provide a cool environment similar to air conditioner environment. If we can control devices base on these condition, it will save energy for the world as well as money for organization.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data, AWS Lambda and API Gateway for processing, DynamoDB for data storage, Amazon S3 for static website hosting and Amazon Cognito ensures secure access, AWS SNS to send notifications when system monitoring device or when supisious weather condition detect. Similar to Thingsboard and CoreIoT, users can register new devices and manage connections, though this platform operates on a smaller scale and is designed for private use. Key features include real-time monitoring, weather prediction, auto controlling and low operational costs.\nBenefits and Return on Investment The Smart Office system enhances automation, monitoring, and energy efficiency across office environments, providing a reliable platform for research, expansion, and real-world IoT application development. It serves as a foundational resource for lab members and developers to build advanced smart environment solutions, while offering a centralized system that reduces manual configuration, improves data reliability, and simplifies maintenance.\nMonthly operating costs are estimated at $1.81 USD, including DynamoDB ($0.09), IoT Core ($0.18), CloudFront ($1.27), CloudWatch ($0.25) and SNS ($0.02), totaling $21.72 USD per year. As IoT hardware is already deployed, there are no additional equipment expenses. The system achieves a break-even period of 6–12 months through significant time savings and reduced manual operation, offering long-term scalability and cost efficiency.\n3. Solution Architecture The Smart Office system adopts a fully serverless AWS architecture optimized for cost efficiency and scalability. Data from multiple sensor hubs is transmitted to AWS IoT Core, processed by Lambda functions, and stored in DynamoDB for real-time monitoring and configuration management. EventBridge automates scheduled device actions and anomaly detection, while SNS handles system notifications. The web dashboard is hosted on S3 and delivered securely via CloudFront, with user authentication managed through Amazon Cognito. This architecture minimizes operational overhead and ensures high reliability for smart environment control.\nAWS Services Used AWS IoT Core: Ingests and manages MQTT data from eight smart room hubs, enabling near real-time monitoring and automation. AWS Lambda: Handles sensor data processing, configuration updates, and automation triggers (four primary functions). Amazon API Gateway: Connects the web client to backend services through secure RESTful APIs. Amazon DynamoDB: Stores user profiles, office and room configurations, and recent sensor logs for fast access. Amazon EventBridge: Triggers scheduled automation events and anomaly detection workflows. Amazon SNS: Delivers alerts and notifications when anomalies or threshold events occur. Amazon S3: Hosts the web dashboard and static assets for the Smart Office application. Amazon CloudFront: Distributes the web dashboard globally with secure HTTPS access and low latency. Amazon Cognito: Manages authentication and authorization for users interacting with the system. Amazon CloudWatch: Monitors metrics and stores operational logs for debugging and performance optimization. Component Design Sensor Hubs: Each IoT-enabled room device collects environmental data (temperature, humidity, light, etc.) and sends it to AWS IoT Core every two minutes. Data Ingestion: AWS IoT Core routes incoming MQTT messages to the SensorProcessor Lambda, which validates and logs the data into Amazon DynamoDB. Configuration Management: The RoomConfigHandler Lambda updates room settings (auto/manual modes, thresholds, timers) in DynamoDB when users make changes through the dashboard. Automation Control: AutomationSetup Lambda listens to DynamoDB Streams for configuration updates and registers automation events in Amazon EventBridge. Event Handling: EventBridge triggers AutomationHandler Lambda at scheduled times or when anomalies are detected, sending alerts via Amazon SNS if needed. User Interaction: The web dashboard (hosted on Amazon S3 and delivered via CloudFront) allows users to view sensor data and manage configurations. User Authentication: Amazon Cognito secures user access with sign-up, sign-in, and token-based authorization integrated through API Gateway. Monitoring \u0026amp; Reliability: Amazon CloudWatch collects logs and metrics from all Lambda functions, with SQS queues as DLQs to capture failed executions for debugging. 4. Technical Implementation Implementation Phases\nBuild Theory and Research AWS Services: Study core AWS components (IoT Core, Lambda, S3, API Gateway, EventBridge, CloudFront, Cognito) and learn their integration for IoT data handling and automation (7 weeks). Design Architecture and Estimate Costs: Create the serverless architecture diagram for an 8-room smart office and use the AWS Pricing Calculator to forecast monthly expenses (1 week). Develop and Optimize Solution: Implement scripts that simulate IoT sensor data to send to AWS IoT Core, build Lambda functions for automation and anomaly detection, and connect the web dashboard with API Gateway and DynamoDB (3 weeks). Testing and Deployment: Deploy all AWS services, test system reliability and rule triggers, validate message flows from IoT Core through EventBridge and Lambda, and monitor real-time results on the hosted dashboard (1 week). Technical Requirements\nSensor Hub Network: Each room includes a Sensor Hub with temperature, humidity, and light sensors, controlled by an ESP32 or similar microcontroller. Hubs send sensor data every 2 minutes via MQTT to AWS IoT Core when in auto mode. Smart Office Platform: Implements a serverless AWS architecture using AWS IoT Core (data ingestion and rule engine), AWS Lambda (data processing and automation logic), Amazon DynamoDB (device configuration and user data), Amazon S3 (web hosting), Amazon API Gateway (REST API), Amazon EventBridge (scheduled automation and anomaly detection), Amazon SNS (notifications), Amazon CloudFront (content delivery), and Amazon Cognito (user authentication). Deployment and Tools: All services are provisioned via AWS CDK/SDK. The web dashboard, built with Next.js and hosted on S3 + CloudFront, allows configuration, real-time monitoring, and automation scheduling while minimizing backend Lambda execution time. 5. Timeline \u0026amp; Milestones Project Timeline\nWeeks 1–7: Study and research AWS services including IoT Core, Lambda, DynamoDB, S3, API Gateway, and Cognito. Week 8: Design the full architecture and estimate costs using the AWS Pricing Calculator. Weeks 9–11: Develop and integrate all system components — data simulation scripts, Lambda functions, database setup, and web dashboard. Week 12: Test, debug, and deploy the system to production. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services:\nAmazon DynamoDB: Free (0.00208 GB/month) AWS Lambda: Free (119,000 requests/month, 13,386.25 GB/s) AWS IoT Core: $0.18/month (8 devices, 13,000 messages/month) AWS API Gateway: Free (720 requests/month) Amazon Simple Storage Service (S3): Free (0.01 GB) Amazon CloudFront: $1.27/month (10,000 requests/month) Amazon EventBridge: Free (2600 events/month) Amazon Simple Queue Service (SQS): Free (2600 requests/month) Amazon CloudWatch: $0.25/month (0,3612736 GB/month) (set retention 3 days) Amazon Amazon Simple Notification Service (SNS): $0.02/month (2100 requests/month, 2100 emails/month) Amazon Cognito: Free (3 MAU/month) Hardware: Existing smart office sensors and hubs — no additional cost. Total: ≈ $1.81/month, or $21.72/year (within AWS Free Tier limits). 7. Risk Assessment Risk Matrix IoT Connectivity Issues: Medium impact, medium probability. Sensor Hub Malfunction: High impact, low probability. Unexpected AWS Charges: Medium impact, low probability. Data Inconsistency or Delay: Medium impact, medium probability. Mitigation Strategies Connectivity: Implement message buffering at Sensor Hub; retry on reconnect. Hardware: Keep spare hubs and perform periodic health checks. Cost: Use AWS Budgets and Cost Explorer to monitor spending within Free Tier. Data Reliability: Validate incoming IoT data via Lambda before storing in DynamoDB. Contingency Plans Switch to manual operation if IoT service disruption occurs. Enable CloudWatch alerts for early issue detection. Use AWS CloudFormation or CDK rollback for quick recovery and cost control. 8. Expected Outcomes Technical Improvements: Real-time monitoring and automation replace manual room control. Centralized platform enhances data accuracy and consistency. Scalable architecture supports future expansion to more rooms or other IoT devices. Long-term Value: Provides a reusable foundation for smart building and IoT automation research. Enables data-driven insights for energy efficiency and environmental optimization. Demonstrates a fully serverless, low-cost AWS architecture (under $2/month). Proposal Link Smart_Office_Proposal\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Learn Gitlab Config AWS Lambda Functions for web client Deploy web client from Gitlab Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn to use GitLab and Gitlab Pipeline 11/10/2025 11/10/2025 https://www.youtube.com/watch?v=bnF7f1zGpo4 3 - Migrate workshop client website repository from GitHub to GitLab 11/11/2025 11/11/2025 4 - Configure Lambda policies, Lambda role and Lambda function for web client 11/12/2025 11/12/2025 5 - Finish and test Lambda function 11/13/2025 11/13/2025 6 - Adjust Gitlab CI to auto push to S3 Bucket 11/14/2025 11/14/2025 Week 10 Achievements: Know how to use Gitlab and config Gitlab pipeline Finish Lambda Function for client Summarize knowledge to a note website https://programming-note-fawn.vercel.app/ "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Learn JavaScript, TypeScript, Prisma and React Expand client interface functions Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Participate in event AWS Cloud Mastery Series #2 11/17/2025 11/17/2025 3 - Learn JavaScript and TypeScript basic content 11/18/2025 11/18/2025 4 - Learn Prisma and React - Practicing on personal project 11/19/2025 11/19/2025 https://photo-locker.vercel.app/ 5 - Draw client function page for admin and manager 11/20/2025 11/20/2025 6 - Expand client interface and functions 11/21/2025 11/21/2025 Week 11 Achievements: Know some basic content of JavaScript and TypeScript Know React framework and Prisma Object-Relational Mapping Summarize knowledge to a note website https://programming-note-fawn.vercel.app/ "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Set up CloudFormation templates Tasks to be carried out this week: | Day | Task | Start Date | Completion Date | Reference Material | | \u0026mdash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; | | | 2 | - Set up CloudFormation templates for Budget, S3 + CloudFront | 11/24/2025 | 11/24/2025 | | | 3 | - Set up CloudFormation templates for DynamoDB and Cognito | 11/25/2025 | 11/25/2025 | | | 4 | - Set up CloudFormation templates for Authenticate and Readonly lambda | 11/26/2025 | 11/26/2025 | | | 5 | - Set up CloudFormation templates for Crud and IoT lambda | 11/27/2025 | 11/27/2025 | | | 6 | - Set up CloudFormation templates for IoT Core and Api Gateway | 11/28/2025 | 11/28/2025 | |\nWeek 12 Achievements: Finish basic CloudFormation templates for deploying website "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/1-worklog/1.13-week13/",
	"title": "Week 13 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 13 Objectives: Adjust CloudFormation Update API Write workshop detail Tasks to be carried out this week: | Day | Task | Start Date | Completion Date | Reference Material | | \u0026mdash; | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;- | \u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash;\u0026mdash; | | | 2 | - Update API to link with frontend | 12/01/2025 | 12/01/2025 | | | 3 | - Test and fix website bugs | 12/02/2025 | 12/02/2025 | | | 4 | - Write script to send and receive data by mocking an IoT device with Laptop | 12/03/2025 | 12/03/2025 | | | 5 | - Write workshop detail | 12/04/2025 | 12/04/2025 | | | 6 | - Finish workshop | 12/05/2025 | 12/07/2025 | |\nWeek 13 Achievements: Finish workshop "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/4-eventparticipated/4.2-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AWS Cloud Mastery Series #2: ​DevOps on AWS\u0026rdquo; Event Objectives DevOps Mindset AWS DevOps Services – CI/CD Pipeline Infrastructure as Code (IaC) Container Services on AWS Monitoring \u0026amp; Observability DevOps Best Practices \u0026amp; Case Studies Speakers Truong Quang Tinh Nghiem Le Long Huynh Quy Pham Key Highlights DevOps Mindset Collaboration and shared responsibility Automation everything Continuos learning and experimentaion Measurement DevOps Journey Do: + Start with dundamentals + Learn by buiding real projects + Document everything + Master one thing at a time + Soft skills enchancement\nDont + Stay in tutorial hell + Copy-paste blindly + Compare your progress to others + Give up after failures\nCI/CD pipeline Application life cycle: from coding, testing, review, pre-prod to real production evironment Infrastructure as Code (IaC) Use code, not clicks, to manage cloud resources. Automatically create, update, and delete infrastructure. Tool: TerraForm, OpenTofu, Pulumi Container Services on AWS Manage container with Docker, Kubernetes, Amazon ECR and Amazon EKS Amazon AppRunner - A fast, simple, and cost-effective AWS service to deploy web applications directly from source code withno need to manage servers or configure AWS infrastructure Monitoring \u0026amp; Observability Best practices with Amazon CloudWatch and Amazon X-Ray Key Takeaways DevOps metrics Monitoring deployment health Improve agility Ensure system stability Optimize customer experience Justify technology investments All the continuous in CI/CD pipeline Full path of CI/CD pipeline proccess Infrastructure as Code (IaC) in AWS Using Amazon CloudFormation to create template with defined service and parameters Applying to Work Build a roadmap career for DevOps Aplly CI/CD pipleline to current project Build template for project to reduce human errors Event Experience Attending the “​DevOps on AWS” workshop was extremely valuable, giving me a guide for DevOps career with tool and knowlegde. Key experiences included:\nLearning from highly skilled speakers FACJ experts shared their experience on DevOps job Demo monitoring and observability in real project Explore CI/CD pipeline and Infrastructure as Code (IaC) Learn how large companies updated there production continuously throuyh the CI/CD pipeline Learn how to constructs, reusable patterns, and language support Deploying with CloudFormation and CDK Monitoring and observability Apply alerting, dashboards, and on-call processes Some event photos Overall, the event provide full view of DevOps career. More and more, it give some best practice on how to apply CI/CD, monitoring and observability.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.3-run-cloudformation-stack/",
	"title": "Set up cloudformation",
	"tags": [],
	"description": "",
	"content": "Download resources Download these CloudFormation template files:\nsmart_office_budget.yaml smart_office_s3_cloudfront.yaml smart_office_cognito.yaml smart_office_dynamodb.yaml smart_office_lambda_authenticate_with_dynamodb_cognito.yaml smart_office_lambda_readonly_with_dynamodb.yaml smart_office_lambda_crud_with_dynamodb_cognito.yaml smart_office_lambda_crud_with_dynamodb_iot.yaml smart_office_iot_core.yaml smart_office_api_gateway.yaml Deploy CloudFormation Stacks In AWS Management Console, search and choose CloudFormation Click Create stack For Prepare template, check Choose an existing template For Template source, check Upload a template file Click Choose file Choose file smart_office_budget.yaml Click Next For Stack name, enter SmartOffice-Budget-Dev Click Next Add Tags for cost and operation management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) For Stack failure options, check Preserve successfully provisioned resources (To keep created resource for debugging) Click Next Check again and click Submit Do the same for other files with exactly name Template name Stack name smart_office_s3_cloudfront.yaml SmartOffice-S3-CloudFront-Dev smart_office_cognito.yaml SmartOffice-Cognito-Dev smart_office_dynamodb.yaml SmartOffice-DynamoDB-Dev smart_office_lambda_authenticate_with_dynamodb_cognito.yaml SmartOffice-Authenticate-Lambda-Dev smart_office_lambda_readonly_with_dynamodb.yaml SmartOffice-ReadOnly-Lambda-Dev smart_office_lambda_crud_with_dynamodb_cognito.yaml SmartOffice-Crud-Lambda-Dev smart_office_lambda_crud_with_dynamodb_iot.yaml SmartOffice-IoT-Lambda-Dev smart_office_iot_core.yaml SmartOffice-IoT-Core-Dev smart_office_api_gateway.yaml SmartOffice-API-Gateway-Dev "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Jumpstart your cloud career with AWS SimuLearn The blog post “Jumpstart your cloud career with AWS SimuLearn” introduces AWS SimuLearn, an interactive learning platform that combines Generative AI with realistic customer simulations. It allows learners to build both technical and communication skills in a safe, feedback-driven environment, with training tailored by role or industry. Through the stories of Hetvi, Karishma, and Kattie, the post highlights how SimuLearn helps users strengthen customer engagement skills, deepen technical expertise, and gain industry-specific knowledge in areas like healthcare. With over 200 training modules and simulations, AWS SimuLearn empowers early-career cloud professionals to grow their AWS knowledge, boost confidence, and accelerate their cloud career readiness.\nBlog 2 - How to Set Up Automated Alerts for Newly Purchased AWS Savings Plans The blog post “How to Set Up Automated Alerts for Newly Purchased AWS Savings Plans” explains how to implement an automated monitoring and alerting system for newly purchased AWS Savings Plans with low utilization. The solution leverages AWS CloudFormation to create Step Functions, SNS topics, EventBridge schedulers, and necessary IAM roles. The deployment is split into two parts:\nMember Account: hosts a Step Function that checks Savings Plans purchased within the last 7 days and the current month, sending email alerts if utilization is below a defined threshold. Management Account: provides IAM roles and access so the Step Function can analyze Savings Plans across the organization. Users can configure thresholds, scheduling frequency, and alert recipients. Upon receiving alerts, FinOps teams should review utilization data and consider returning underused Savings Plans if eligible. This solution helps FinOps teams proactively manage cloud costs, detect underutilized commitments, and improve overall Savings Plans utilization efficiency across AWS environments. Blog 3 - Operating BYOL Windows Server Workloads Effectively on AWS This AWS blog explains how to effectively operate Bring Your Own License (BYOL) Windows Server workloads on AWS to reduce costs. It covers eligibility requirements for BYOL licenses, preparing Windows Server images using VM Import/Export and Migration Hub Orchestrator, and managing license type conversions with AWS License Manager. The post also shows how to detect configuration issues using AWS Config custom rules and analyze cost data with AWS Cost and Usage Reports (CUR). By following these best practices, organizations can optimize Windows Server costs, ensure license compliance, and improve operational efficiency on AWS.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/4-eventparticipated/4.2-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AWS Cloud Mastery Series #3: ​Theo AWS Well-Architected Security Pillar\u0026rdquo; Event Objectives Security Foundation Identity \u0026amp; Access Management Detection Infrastructure Protection Data Protection Incident Response Speakers Le Vu Xuan An Tran Duc Anh Tran Doan Cong Ly Danh Hoang Hieu Nghi Thinh Lam Viet Nguyen Mendel Branski (Long) Key Highlights Introduce to Cloud Club Introduce some Cloud Club at university like UTE, SGU Activities at Cloud Club Security Foundation Service Control Policies Permission Bondaries Multi-Factor Authentication (MFA) Detection and monitoring Multi-Layer Security Visibility Alerting \u0026amp; Automation with EventBridge Detection-as-code Network and data protection VPC Security Group Sharing API-Based Services Secret Management Incident response Prevention - Nobody has time for incidents Guide to sleeping better Incident response process Key Takeaways Service Control Policies Organizational policy Control maximum available permissions for all account in the organization Never grant permissions, it can only filtered Permission Boundaries Advanced IAM feature designed to solve the sole central security problem Sets the maximum permissions that identity-based policy can grant to an User/Role Multi-Factor Authentication (MFA) TOTP: Shared secret, requires manually 6-digits code (Google Authenticator), free, flexible backups and recovey FIDO2: Public-key cryptography, requires a simple touch biometric scan, variable, strict backups and recovey Alerting \u0026amp; Automation with EventBridge Real-time Events: CloudTrail events flow to EventBrige for immediate processing Automated Alerting: Detect suspicious activities across all organization accounts Cross-account Event Routing: Centralized event processing and automated response Itegration \u0026amp; Workflows: SNS, Stack, SQS integration for automated security response and team notifications Detection-as-Code CloudFormation/Terraform: Deploy GuardDuty across organization with IaC (enable protection plans, configure data sources) Custom Detection Rules: Build suppression rules \u0026amp; IP whitelists to reduce false positives and adapt to your environment Infrastructure-as-Code: Automated org-wide GuardDuty setup and protection plan rollout Version-Controlled Logic: Track detection rules in Git to DevSecOps pipeline integration for rule testing \u0026amp; deployment Incident response Prepare automation handler for incident Predict future incident and design plan for response Lesson learned after each incidents Applying to Work Specify least privilege policy for project Apply MFA for every accounts Predict and prepare for future incident Event Experience Attending the “​Theo AWS Well-Architected Security Pillar” workshop helps me improve my knowleagde in security and incident response through experiencing AWS Security Pillars, include:\nLearn from skilfully speaker Learn how senior handle when incident happen and lesson learned after Learn to protect data and network with AWS security features Explore some Cloud Club activites Introduce to Cloud Club activities thats help connect AWS Learner from everywhere Alerting \u0026amp; Automation Prepare infrastructure like CloudTrail, EventBrigde, CloudWatch to manage resource in real-time as soon as incident occure Some event photos The event is a chance for me to expand my knowledge in Alerting, Automation, Security and Incident Response. I have gained a lot of experience through listening to senior Cloud Engineer talking about there work.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI-Driven Development Life Cycle: Reimagining Software Engineering\nDate \u0026amp; Time: 14:00, October 03, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: Introduction to how AI tools such as Amazon Q Developer and Kiro transform the software development process. Focus on two key methodologies — AI-Driven Development Lifecycle (AI-DLC) emphasizing automation, speed, and scalability, and Spec-Driven Development (SDD) with Kiro emphasizing precision and structure. Presentation of the evolving role of AI in IT, where AI assists or manages development tasks while human validation remains essential.\nOutcome: Better understanding of integrating AI into software engineering while maintaining human-centered validation. Clear insight into the strengths and limitations of AI-DLC and SDD. Exposure to tools like Amazon Q Developer and Kiro, with practical applications for improving efficiency, quality, and innovation in real-world projects.\nEvent 2 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee Description: Introduction to Foundation Models and Generative AI capabilities using Amazon Bedrock. The talk focused on effective prompting techniques (Chain of Thought) and the architecture of Retrieval-Augmented Generation (RAG) for integrating internal knowledge bases. Detailed exploration of vector embeddings with Amazon Titan and an overview of various AWS AI services.\nOutcome: Gained comprehensive knowledge on reducing AI hallucinations and enhancing response accuracy through RAG and Embeddings. Learned how to apply specific AWS AI services and prompting strategies to real-world projects. The event provided valuable insights into building AI agents and offered opportunities to network with skilled speakers and industry experts.\nEvent 3 Event Name: AWS Cloud Mastery Series #2: ​DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17, 2025 Location: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City Role: Attendee Description: Comprehensive overview of the DevOps mindset, emphasizing collaboration, automation, and shared responsibility. The session covered essential AWS DevOps services including CI/CD pipelines, Infrastructure as Code (IaC) with CloudFormation and Terraform, and container management using Amazon EKS and AppRunner. It also provided a guide on the \u0026ldquo;Do\u0026rsquo;s and Don\u0026rsquo;ts\u0026rdquo; of the DevOps journey and best practices for monitoring and observability.\nOutcome: Gained a clear roadmap for a DevOps career and a deeper understanding of the application life cycle. Learned to implement CI/CD pipelines and utilize IaC templates to minimize human errors. Acquired insights into monitoring deployment health and system stability through real-world case studies and expert demonstrations.\nEvent 4 Event Name: AWS Cloud Mastery Series #3: ​Theo AWS Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee Description: Dive into the AWS Well-Architected Security Pillar, which include Identity \u0026amp; Access Management, Infrastructure Protection, and Data Protection. The session explored foundational security measures like Service Control Policies (SCPs), Permission Boundaries, and MFA. It also highlighted advanced strategies for \u0026ldquo;Detection-as-Code\u0026rdquo; using IaC tools and automated incident response workflows with Amazon EventBridge.\nOutcome: Learned how to enforce least privilege principles effectively using SCPs and IAM boundaries. Gained practical skills in setting up automated alerting and cross-account event routing for real-time threat detection. Acquired valuable insights into incident response processes, emphasizing prevention and automation to minimize downtime and security risks.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/5.4-set-up-website/",
	"title": "Set up website",
	"tags": [],
	"description": "",
	"content": "Set up Gitlab repository to deploy website and lambda code Go to this Gitlab repository: https://gitlab.com/tranngockhiet22062005/smart-office Download and deploy on your own repository Set up role for Gitlab to deploy website to S3 and deploy code to Lambda Function Create an IAM User with following attribute (view 5.2 if you forget) User name: smart-office-gitlab-ci Policy: {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;S3ListBucketAccess\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:GetBucketLocation\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::fcj-smart-office-frontend-ACCOUNT_ID-dev\u0026#34;,\r\u0026#34;arn:aws:s3:::fcj-smart-office-lambda-ACCOUNT_ID-dev\u0026#34;\r]\r},\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;S3ReadWriteAccess\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:PutObjectAcl\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: [\r\u0026#34;arn:aws:s3:::fcj-smart-office-frontend-ACCOUNT_ID-dev/*\u0026#34;,\r\u0026#34;arn:aws:s3:::fcj-smart-office-lambda-ACCOUNT_ID-dev/*\u0026#34;\r]\r},\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;LambdaUpdateAccess\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;lambda:UpdateFunctionCode\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetFunctionConfiguration\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:*:*:function:SmartOffice-*\u0026#34;\r},\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;CloudFrontInvalidation\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudfront:CreateInvalidation\u0026#34;,\r\u0026#34;cloudfront:GetDistribution\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} You should replace ACCOUNT_ID with your AWS Account ID\nPolicy name: SmartOfficeGitlabAccess Navigate to the user In Summary, click Create access key For Use case, check Command Line Interface (CLI) Check I understand the above recommendation and want to proceed to create an access key. Click Next Click Create access key Click Download .csv file Config Gitlab variables Go to your Gitlab repository, click Setting \u0026gt; Variables Click Add variable For each Key and Value follow the table bellow to know where to get value Key Value AWS_ACCESS_KEY_ID In your smart-office-gitlab-ci_accessKeys.csv you have downloaded AWS_DEFAULT_REGION ap-southeast-1 (or anywhere you deploy the workshop) AWS_SECRET_ACCESS_KEY In your smart-office-gitlab-ci_accessKeys.csv you have downloaded CLOUDFRONT_DISTRIBUTION_ID CloudFront \u0026gt; Distributions \u0026gt; Your distribution ID S3_BUCKET_FRONTEND fcj-smart-office-frontend-ACCOUNT_ID-dev (replace ACCOUNT_ID with your AWS Account ID) S3_BUCKET_LAMBDA fcj-smart-office-lambda-ACCOUNT_ID-dev (replace ACCOUNT_ID with your AWS Account ID) STACK_NAME_AUTH SmartOffice-Authenticate-Lambda-Dev STACK_NAME_CRUD SmartOffice-Crud-Lambda-Dev STACK_NAME_IOT SmartOffice-IoT-Lambda-Dev STACK_NAME_READONLY SmartOffice-ReadOnly-Lambda-Dev VITE_API_BASE_URL API Gateway \u0026gt; SmartOffice-API-Gateway-Dev-Api \u0026gt; Stages \u0026gt; Invoke URL Push code into an init branch Merge branchs in this order: init -\u0026gt; dev, dev -\u0026gt; main "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Secure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 06/08/2025 to 12/24/2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in Smart Office Project, through which I improved my skills in utilizing AWS Services, teamwork, CI/CD pipeline, web programming.\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ✅ ☐ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Improve the amount of time need to asorbing new knowlegde Enhance communication skill to help connect frontend and backend Presenting idea more clearly to avoid missunderstand "
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment The working environment is very friendly, tidy, and clean. Mentors are always available in the office to answer our questions.\n2. Support from Mentor / Team Admin The mentors provide guidance and very detailed feedback on project structure and architecture diagrams. They are also super enthusiastic about suggesting ways to save costs and improve product performance.\n3. Relevance of Work to Academic Major Thanks to the sharing sessions from seniors, I\u0026rsquo;ve found a clearer direction for my development and am now pursuing the path to become a DevOps Engineer and Cloud Engineer.\n4. Learning \u0026amp; Skill Development Opportunities During the internship, I picked up a lot of new skills, such as using project management tools, time management, and how to communicate professionally in a corporate setting.\n5. Company Culture \u0026amp; Team Spirit AWS culture is very open, always welcoming and updating the latest methods and technologies. Team members are always around and ready to support each other 24/7.\n6. Internship Policies / Benefits The sharing sessions from industry seniors really helped a lot during the project implementation, and they were also valuable lessons for my personal growth and career path.\nAdditional Questions What did you find most satisfying during your internship? There are many sharing sessions from industry seniors, and the mentors are incredibly supportive.\nWhat do you think the company should improve for future interns? None.\nIf recommending to a friend, would you suggest they intern here? Why or why not? Yes, because Cloud Computing is currently the trend, and AWS is leading that trend.\nSuggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? None.\nWould you like to continue this program in the future? Not at the moment, as the team didn\u0026rsquo;t quite meet my expectations for a long-term commitment, but I will definitely continue honing my AWS knowledge.\nAny other comments (free sharing):\nNone.\n"
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://tranngockhiet.github.io/tran-ngoc-khiet-fptse192015-internship-report/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]