[{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":"Risk management analytics: Turning uncertainty into opportunity by Shaheen Kanda | on 29 APR 2025 | in AWS Marketplace, AWS Partner Network, Case Study, Customer Solutions, Financial Services, Industries, Intermediate (200), Partner solutions | Permalink | Share\nToday\u0026rsquo;s financial services landscape requires sophisticated enterprise risk management approaches to address evolving regulations, market volatility, and shifting customer expectations. Outdated systems and fragmented data create barriers to effective risk oversight, preventing institutions from gaining comprehensive insights.\nIn a recent webinar, Risk management analytics: Turning uncertainty into opportunity, we explored how financial institutions are reimagining risk analytics through digital transformation with solutions available in AWS Marketplace. We also discussed strategies that convert risk functions from defensive necessities into competitive advantages.\nCraig Vogel, Associate Vice President, Cross Asset Derivatives Trader at Fortitude Re, and Arsen Arutyunov, Vice President of Sales – Americas at Numerix, shared their experiences with risk transformation initiatives.\nNavigating regulatory complexity in financial services Industry experts from Deloitte and Ernst \u0026amp; Young emphasize that effective risk frameworks begin with strong governance. These leaders underscore the need for financial services firms to develop adaptive strategies that integrate comprehensive cybersecurity measures, anticipate evolving customer expectations, and create resilient approaches to emerging market uncertainties.\nProgressive organizations move beyond reactive approaches with analytics-driven methodologies. They deploy integrated solutions that unify information sources, deliver timely insights, and facilitate predictive modeling. This shift positions risk management as a value creator rather than merely a compliance function.\nReal-world examples of customer success Enhancing customer intelligence with SAS Toyota Financial Services Italia recognized the need to reimagine their approach to customer data, which was previously trapped in disconnected silos that limited their ability to personalize experiences and manage risk effectively. They turned to SAS for support.\nBy implementing SAS Viya, Toyota Financial Services Italia reinvented their data strategy, centralizing fragmented customer information into a secure, unified repository. This transformative approach allowed them to completely reimagine how they view and understand their customers.\nLeveraging advanced machine learning capabilities, their Information Technology (IT) analytics department launched innovative predictive models to uncover hidden patterns. These models transformed isolated data points into a comprehensive intelligence platform, enabling the team to reimagine the potential of their customer insights.\nThe reimagined approach yielded impressive results:\nImproved customer experience through data-driven personalization Targeted retention strategies that predict customer churn with unprecedented accuracy Foundations for comprehensive risk management Innovative pathways for addressing evolving compliance requirements Toyota is now poised to further reinvent their business intelligence, with plans to develop advanced models for risk forecasting and capital allocation. They are building a visionary data culture where precise, reimagined information supports every critical business decision through the SAS Viya platform.\nRevolutionizing compliance workflows with Smarsh A leading global financial services company managed regulatory compliance with multiple vendors for electronic communications. This approach wasted time and money while limiting their ability to extract insights from communication data.\nSmarsh Enterprise Platform consolidated their company\u0026rsquo;s solution and transformed their compliance operations. By switching to a single vendor, the company gained significant benefits across compliance, IT, and investigations teams.\nThe implementation delivered impressive results:\n$7 million in annual cost savings through vendor consolidation Significantly improved investigation efficiency 70% reduction in false positive alerts through AI-powered surveillance Expanded foreign language coverage for global compliance New revenue opportunities from communication data insights Transforming derivative trading operations with Numerix Fortitude Re built a comprehensive risk management system with Numerix that spans multiple asset classes. Starting with just three employees managing variable annuities, they needed analytics beyond their internal capabilities.\n\u0026ldquo;By having the liability move in Numerix, we can produce a tighter hedge as opposed to hedging from beginning-of-day risk,\u0026rdquo; explains Craig Vogel. \u0026ldquo;I can hedge my liability risk as of 3:00 PM EDT and reduce the mismatch with high accuracy.\u0026rdquo;\nThe implementation consolidated trade operations and ensured accurate data synchronization across front and middle offices. The team now accesses intraday data to quickly assess positions and execute hedges throughout trading hours.\nLeveraging cloud-native solutions for enhanced infrastructure security Numerix\u0026rsquo;s cloud-native solution on Amazon Web Services (AWS) eliminated Fortitude Re\u0026rsquo;s need to hire 15-20 people to manage infrastructure. The Software as a Service (SaaS) approach provides a secure, controlled environment with strict security standards matching Fortitude Re\u0026rsquo;s own requirements.\nThe single-tenant SaaS offering provides enhanced controls through a two-person approval matrix for trade verification. This system maintains strict permissions and prevents unauthorized manipulation, to ensure data integrity and security.\n\u0026ldquo;From their perspective, they\u0026rsquo;re getting an immensely powerful seamless user experience,\u0026rdquo; explains Arsen Arutyunov. \u0026ldquo;So, for them it\u0026rsquo;s just a streamlined experience. That\u0026rsquo;s really the goal here.\u0026rdquo;\nExpert perspectives on risk transformation The webinar featured insights from practitioners who have successfully navigated risk management modernization. Their discussion revealed valuable lessons for organizations embarking on similar journeys:\nThe critical role of executive support for transformation initiatives Practical strategies for balancing innovation with regulatory compliance Methods for maintaining data security while enhancing analytical capabilities Approaches for demonstrating risk management value These experts shared candid perspectives on challenges and success factors for deploying risk management solutions available in AWS Marketplace.\nConclusion Financial services organizations gain significant advantages when they modernize their risk capabilities. The case studies of Toyota Financial Services Italia with SAS, a global financial institution with Smarsh, and Fortitude Re with Numerix demonstrate the power of analytics.\nThese organizations centralized data, implemented real-time analytics, and automated key processes. They reduced costs while improving decision quality and gained better visibility into risk exposures. Most importantly, they transformed risk management into a strategic function that creates business value.\nAWS Marketplace provides access to these and many more powerful risk management solutions, enabling financial institutions to implement proven technologies using streamlined procurement.\nAbout AWS Marketplace AWS Marketplace is a digital catalog of third-party software, services, and data that simplifies how financial services and insurance organizations find, buy, deploy, and manage software on AWS. With over 20,000 partner offerings across more than 70 categories deployed in 31 regions globally, AWS Marketplace helps customers transform their software supply chain.\nAWS Marketplace offers flexible pricing options including free trials, pay-as-you-go, and enterprise discount programs. Multiple deployment options help customize software provisioning to fit security policies and licensing terms. According to Forrester, finding, buying, and deploying solutions through AWS Marketplace takes approximately half the time compared to other sales channels.\nVisit Financial Services Solutions in AWS Marketplace to discover a variety of financial services and insurance solutions like the ones discussed from Numerix, SAS, and Smarsh.\nWatch Risk management analytics: Turning uncertainty into opportunity to learn more about these topics or discover additional insights from the panel discussion.\nAbout the author Shaheen Kanda serves as the Financial Services Technology Partnerships and Cross Industry Marketplace Leader at AWS. She works with financial services organizations to identify and implement innovative solutions that address their unique challenges, with a particular focus on risk management, compliance, and digital transformation. Before joining AWS, she founded two FinTech startups and spent a decade working as a leveraged finance banker in New York City.\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"Workshop Report: AWS Cloud Mastery Series #1 — AI/ML/GenAI on AWS Date: Saturday, November 15, 2025\nTime: 8:30 AM – 12:00 PM\nLocation: Bitexco Financial Tower, Ho Chi Minh City\nHost: Kha Van\nAttendees: 348 participants\nOverview The first session of the AWS Cloud Mastery Series introduced modern machine learning and generative AI approaches on AWS, with a focus on practical applications and hands-on demonstrations. The workshop covered both SageMaker for traditional ML workflows and Amazon Bedrock for building with foundation models.\nSession Breakdown 8:30 – 9:00 AM | Welcome \u0026amp; Introduction The session opened with participant registration and networking, followed by an overview of the AI/ML landscape in Vietnam. The organizers set expectations for the day\u0026rsquo;s learning objectives and conducted an ice-breaker activity to encourage collaboration among the 348 attendees.\n9:00 – 10:30 AM | AWS AI/ML Services Overview This segment introduced Amazon SageMaker as a comprehensive end-to-end ML platform:\nData Preparation \u0026amp; Labeling: Techniques for preparing datasets and labeling for supervised learning Model Training \u0026amp; Tuning: Hyperparameter optimization and AutoML capabilities Deployment: Moving trained models to production with integrated MLOps support Live Demo: A hands-on walkthrough of SageMaker Studio, showing how to launch experiments and monitor training jobs Key takeaway: SageMaker eliminates the need to manage underlying infrastructure while providing full control over model training and deployment.\n10:30 – 10:45 AM | Coffee Break 10:45 AM – 12:00 PM | Generative AI with Amazon Bedrock The second half focused on building with foundation models using Amazon Bedrock:\nFoundation Models Overview Claude, Llama, Titan Comparison: Different models for different use cases (reasoning, creative, multilingual) Model Selection Guide: How to choose the right model based on latency, cost, and accuracy requirements Prompt Engineering Techniques Chain-of-Thought Reasoning: Breaking down complex problems into step-by-step workflows Few-Shot Learning: Using examples to guide model behavior Advanced Prompting: Patterns for extracting structured data and controlling output format Retrieval-Augmented Generation (RAG) Architecture \u0026amp; Integration: Connecting external knowledge bases to Bedrock Knowledge Base Integration: Keeping models up-to-date with proprietary data Reducing Hallucinations: Using retrieval to ground model responses in facts Bedrock Agents Multi-Step Workflows: Automating complex business processes Tool Integration: Connecting models to databases, APIs, and other services Error Handling: Graceful degradation and retry logic Safety \u0026amp; Compliance Guardrails: AWS\u0026rsquo;s content filtering and safety mechanisms Output Filtering: Preventing harmful or inappropriate responses Live Demo: Building a Generative AI Chatbot The instructors demonstrated a complete RAG-powered chatbot on Bedrock, showing:\nDocument upload and knowledge base indexing Query processing with retrieval Model inference with guardrails applied Real-time response generation Personal Reflections What I Learned The workshop clarified the distinction between traditional ML (SageMaker) and generative AI (Bedrock). Traditional ML remains essential for classification, regression, and time-series forecasting, while Bedrock excels at text understanding, generation, and reasoning tasks.\nThe practical emphasis on prompt engineering and RAG resonated particularly—these techniques are immediately applicable and require no machine learning expertise to implement.\nKey Insights Foundation models are production-ready: AWS has invested heavily in security, compliance, and scaling for enterprise workloads. RAG is essential for enterprise AI: Organizations cannot rely on foundation models alone; knowledge integration is critical for accuracy. Prompt engineering is a real skill: Well-crafted prompts and few-shot examples can dramatically improve model output quality. Vietnam\u0026rsquo;s AI landscape is growing: With 348 participants, there\u0026rsquo;s clear momentum in the local AI community. Next Steps Experiment with Bedrock\u0026rsquo;s prompt playground for internal projects Design a RAG architecture for our team\u0026rsquo;s knowledge base Evaluate which use cases benefit from foundation models vs. traditional ML Consider AWS\u0026rsquo;s SageMaker Canvas for low-code model building Some pictures from the event This workshop was an excellent foundation for understanding modern AI on AWS. The combination of theory, live demos, and hands-on exploration created a comprehensive introduction to both traditional ML and generative AI workflows.\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Vo An Khang\nPhone Number: 0966497517\nEmail: voankhangpt@gmail.com\nUniversity: FPT University Ho Chi Minh Campus\nMajor: Information Assurance\nClass: AWS\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FACJ Cloud Intern\nInternship Duration: From 09/06/2025 to 12/24/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Serverless \u0026amp; Event-Driven Architecture Serverless Architecture: This workshop utilizes a cloud-native model with services like AWS Lambda, Amazon API Gateway, and Amazon DynamoDB. This approach allows code to run in response to requests without provisioning or managing servers, as AWS handles all automatic scaling and infrastructure management. Event-Driven Architecture: The core of the system functions on an event-driven basis. Instead of services continuously polling for data, specific events—such as IoT sensor readings or user API calls—trigger downstream workflows. This is orchestrated by AWS IoT Core and Amazon EventBridge, creating a highly flexible and scalable system. Workshop overview In this workshop, you will deploy a comprehensive serverless data platform on AWS to manage real-time environmental monitoring for an 8-room smart office setup. The system integrates AWS IoT Core, Lambda, DynamoDB, S3, CloudFront, and Amazon Cognito. Sensor data is forwarded from edge devices (or simulated scripts), ingested into AWS, stored in DynamoDB tables, and processed by Lambda functions to update the management dashboard. Critical events are routed through EventBridge for alerting, demonstrating a high-availability, low-cost, and seamless scalability architecture.\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Team up for the project and understand the workshop rules. Set up a secure AWS environment (Account \u0026amp; IAM). Learn and practice core services: VPC, EC2, and Lambda. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Logistics: + Meet team members and exchange contacts + Read workshop policies and guides 09/08/2025 09/08/2025 https://policies.fcjuni.com 3 - Account Setup: + Create AWS Free Tier Account + Activate Multi-Factor Authentication (MFA) + Create a Billing Alarm (Critical for cost control) 09/09/2025 09/09/2025 https://000001.awsstudygroup.com/ 4 - Networking Basics (VPC): + Create a custom VPC (Virtual Private Cloud) + Create one Public Subnet + Attach an Internet Gateway (IGW) to enable internet access 09/10/2025 09/10/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000003.awsstudygroup.com/ 5 - Compute Basics (EC2): + Launch an EC2 Instance (Linux t2.micro) into your Public Subnet + Connect to the instance using Instance Connect or SSH 09/11/2025 09/11/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html 6 - Serverless Basics (Lambda): + Create a simple \u0026ldquo;Hello World\u0026rdquo; Lambda Function + Test the function manually in the console 09/12/2025 09/12/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://docs.aws.amazon.com/lambda/latest/dg/welcome.html Week 1 Achievements: Successfully teamed up and reviewed workshop protocols. Secured the AWS Root User with MFA and set up billing alerts. Built a foundational network (VPC) from scratch. Launched and accessed a virtual server (EC2). Executed a serverless function (Lambda) successfully. "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/","title":"Worklog","tags":[],"description":"","content":"Internship Worklog Overview Weeks 1-5: AWS Foundations Week 1: AWS Fundamentals \u0026amp; Account Setup Secure AWS account setup with VPC networking, EC2 instances, and Lambda basics.\nWeek 2: Linux, Python, IAM \u0026amp; Databases Linux administration, Python scripting, IAM identity management, and RDS fundamentals.\nWeek 3: Storage, Databases \u0026amp; APIs S3 bucket management, DynamoDB NoSQL databases, and MQTT protocol for IoT.\nWeek 4: Serverless \u0026amp; Event-Driven Architecture Lambda functions, S3 event triggers, API Gateway, and event-driven IoT patterns.\nWeek 5: Messaging Services \u0026amp; Project Preparation SNS topics, EventBridge scheduling, AWS IoT Core overview, and project readiness.\nWeek 6-13: Smart Office IoT Project Implementation Week 6: IoT Core Environment Setup AWS IoT Core initialization, device registration, certificate management, and MQTT connectivity.\nWeek 7: Security \u0026amp; Resilience Deep Dive Enterprise security (IAM, encryption, WAF, GuardDuty), high availability, and disaster recovery.\nWeek 8: Performance, Cost Optimization \u0026amp; Midterm Exam Storage optimization, RDS deployment strategies, cost management, and AWS Well-Architected Framework.\nWeek 9: MQTT Messaging \u0026amp; Data Simulation MQTT topic hierarchy design and Python IoT device simulator with realistic sensor data streaming.\nWeek 10: IoT Rules \u0026amp; EventBridge Integration AWS IoT Rules Engine with SQL filtering, EventBridge anomaly detection, and real-time data processing.\nWeek 11: Debugging, Code Refinement \u0026amp; Testing System debugging, device scheduling implementation, code optimization, and production readiness.\nWeek 12: Linux \u0026amp; Shell Scripting Mastery Linux commands, file manipulation, permissions, and shell scripting for infrastructure automation.\nWeek 13: Final Testing, Workshop \u0026amp; Presentation End-to-end system testing, comprehensive workshop documentation, demo video, and successful project delivery.\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":"Optimizing cold start performance of AWS Lambda using advanced priming strategies with SnapStart by Shan Kandaswamy and Gustavo Martim | on 29 APR 2025 | in Advanced (300), AWS Lambda, Best Practices, Serverless, Technical How-to | Permalink | Share\nIntroduced at re:Invent 2022, SnapStart is a performance optimization that makes it easier to build highly responsive and scalable applications using AWS Lambda. The largest contributor to startup latency (often referred to as cold-start time) is the time spent initializing a function. This includes loading the function\u0026rsquo;s code and initializing dependencies. For latency-sensitive workloads such as APIs and real-time data processing applications, high startup latency can result in a suboptimal end user experience. Lambda SnapStart can reduce startup duration from several seconds to as low as sub-second, with minimal or no code changes. This post discusses \u0026lsquo;Priming\u0026rsquo;, a technique to further optimize startup times for AWS Lambda functions built using Java and Spring Boot.\nSpring Boot applications typically experience high cold start latency during JVM and framework initialization, where significant time is spent loading classes and performing Just-In-Time (JIT) compilation of Java bytecode. This blog post uses a Spring Boot application as an example that retrieves 10 records from a \u0026lsquo;UnicornEmployee\u0026rsquo; table in an Amazon RDS for PostgreSQL database, where each employee record includes employee name, location, and hire date.\nThe sample application uses Amazon API Gateway which triggers an AWS Lambda function that connects to the database through RDS Proxy to return the employee data. While this sample application uses dummy employee data for demonstration, the patterns and optimization techniques discussed in this post are applicable to real-world scenarios with similar data access patterns. Sample code for this implementation can be found in our GitHub repository at lambda-priming-crac-java-cdk.\nBackground: How SnapStart works The post assumes familiarity with SnapStart and provides a short background. For additional details, refer to the SnapStart documentation.\nTo quickly recap, the INIT phase for a Lambda function involves downloading the function\u0026rsquo;s code, starting the runtime and any external dependencies, and running the function\u0026rsquo;s initialization code. For functions that don\u0026rsquo;t use SnapStart, this phase occurs each time your application scales up to create a new execution environment. When SnapStart is activated, the INIT phase happens when you publish a function version.\nThe following image shows a comparison of a Lambda request lifecycle with and without SnapStart.\nFigure 1 – comparison of a Lambda request lifecycle with and without SnapStart\nAt the end of the INIT phase, Lambda executes your before-checkpoint runtime hooks. Lambda then snapshots the memory and disk state of the initialized execution environment, persists the encrypted snapshot, and caches it for low-latency access. When the function is subsequently invoked, new execution environments are resumed from the cached snapshot (during the RESTORE phase), speeding up function startup.\nFigure 2 – new execution environments are resumed from the cached snapshot\nYou can validate this speedup by comparing the RESTORE duration with the INIT duration recorded before SnapStart in your Lambda function\u0026rsquo;s Amazon CloudWatch Logs. As demonstrated in the following table, enabling SnapStart reduces the startup latency of our sample Spring Boot application by 4.3x from 6.1s to 1.4s. The 6.1s cold start latency for ON_DEMAND is primarily due to the combination of (1) initializing the JVM and Spring Boot framework, (2) JIT compilation of lazy loaded application code during initial invocation and (3) the time needed to establish a database connection with RDS through Amazon RDS Proxy. By enabling SnapStart, Lambda initializes the JVM and Spring Boot prior to the function invocation – resulting in the significantly reduced latency of 1.4s.\nMethod Cold Start Invocations p50 P90 P99 p99.9 PrimingLogGroup-1_ON_DEMAND 128 5047.94 ms 5386.78 ms 6158.80 ms 6195.84 ms PrimingLogGroup-2_SnapStart_NO_PRIMING 111 1177.87 ms 1288.73 ms 1419.94 ms 1425.63 ms You can reduce cold starts even further for your latency-sensitive Spring Boot applications by using priming techniques on Lambda functions. Let\u0026rsquo;s explore how to implement priming techniques.\nPriming explained Priming is the process of preloading dependencies and initializing resources during the INIT phase, rather than during the INVOKE phase to further optimize startup performance with SnapStart. This is required because Java frameworks that use dependency injection load classes into memory when these classes are explicitly invoked, which typically happens during Lambda\u0026rsquo;s INVOKE phase. You can proactively load classes using Java runtime hooks, that are part of the open source CRaC (Coordinated Restore at Checkpoint) project. This post demonstrates how to use this hook, called beforeCheckpoint(), to prime SnapStart-enabled Java functions, in two ways:\nInvoke Priming: This approach involves directly invoking application endpoints or methods in your pre-snapshotting hook so that they are JIT compiled during the INIT phase and included in the snapshot. This can include operations such as invoking API Gateway endpoints or fetching data from an S3 bucket or RDS database to proactively execute the code paths, ensuring that the underlying classes are included in the snapshot.\nClass Priming: This approach involves proactive initialization of classes during the INIT phase, ensuring that they are included in the function\u0026rsquo;s snapshot without risking unwanted changes to application state or data. This can be achieved by leveraging Java\u0026rsquo;s forName() method, which loads, links, and initializes the specified class. Initialization refers to the JVM process of loading the class definition into memory, verifying the bytecode, preparing static fields with default values, and executing static initializers. This is different from instantiation, which creates objects of the class using constructors. To generate a list of the classes required for pre-loading, you can use the following VM option, writing the list to a file called classes-loaded.txt: -Xlog:class+load=info:classes-loaded.txt\nWhile invoke priming can offer better performance, it requires additional effort to ensure that the actions performed are idempotent and do not have unintended side effects, for instance processing financial transactions in a banking application. For this reason, invoke priming should only be used when code executed during priming is either idempotent or does not modify state. For scenarios where this is not possible, class priming provides a safer alternative by only initializing classes without executing their methods. Note that this assumes your application does not execute state-modifying code during class initialization.\nWith this context, let\u0026rsquo;s look at how to implement Invoke and Class priming for a Spring Boot sample application.\nExample priming Implementation using CRaC runtime hooks before taking a Lambda snapshot This post demonstrates both Invoke priming and Class priming using the sample Spring Boot application. The choice between the two approaches depends on the specific requirements and complexities of your application.\nStep 1: Set up your Spring Boot Application using the aws-serverless-springboot3-archetype as explained in our Quick Start Spring Boot3 guide, adding database connectivity code, or simply clone the sample project from GitHub repository.\nCreate a Spring Boot Application. // src/main/java/software/amazon/awscdk/examples/unicorn/UnicornApplication.java package software.amazon.awscdk.examples.unicorn; … @Import({ UnicornConfig.class }) @SpringBootApplication public class UnicornApplication { private static final Logger log = LoggerFactory.getLogger(UnicornApplication.class); public static void main(String... arguments) { SpringApplication.run(UnicornApplication.class, arguments); } } Add all the necessary Maven dependencies for Spring Boot, AWS Lambda, and Database Connection in your pom.xml file. The following, highlighted, dependency contains the classes required to use the CRaC runtime hooks. ... \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.crac\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;crac\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; ... Configure Database Connection – Set up the database connection details in application.properties. spring.datasource.password=${SPRING_DATASOURCE_PASSWORD} spring.datasource.url=${SPRING_DATASOURCE_URL} spring.datasource.username=postgres spring.datasource.hikari.maximumPoolSize=1 Step 2: Implement Lambda Function Handler with CRaC runtime hooks and Invoke Priming Approach:\nCreate Lambda Function Handler and integrate CRaC runtime hooks to execute beforeCheckpoint() and afterRestore() methods in your application for before taking and after restoring the snapshot.\nImplement the RequestHandler\u0026lt;UnicornRequest, UnicornResponse\u0026gt; interface in the Lambda function handler class. Implement the CRaC resource interface with two methods: beforeCheckpoint() and afterRestore(), which defines actions performed before Lambda creates the snapshot and after the snapshot is restored. Add invoke priming by creating a UnicornRequest object with a GET request to a specific endpoint (such as, /unicorn) and call the handleRequest(unicornRequest, null) method. This ensures that the code paths associated with the specified endpoint are JIT compiled and optimized for faster execution during the first invocation after the snapshot is restored.\n/src/main/java/software/amazon/awscdk/examples/unicorn/handler/InvokePriming.java package software.amazon.awscdk.examples.unicorn.handler; import org.crac.Core; import org.crac.Resource; ... public class InvokePriming implements RequestHandler\u0026lt;APIGatewayV2HTTPEvent, APIGatewayV2HTTPResponse\u0026gt;, Resource { ... @Override public APIGatewayV2HTTPResponse handleRequest(APIGatewayV2HTTPEvent event, Context context) { var awsLambdaInitializationType = System.getenv(\u0026#34;AWS_LAMBDA_INITIALIZATION_TYPE\u0026#34;); var unicorns = getUnicorns(); var body = gson.toJson(unicorns); return APIGatewayV2HTTPResponse.builder().withStatusCode(200).withBody(body).build(); } @Override public void beforeCheckpoint(org.crac.Context\u0026lt;? extends Resource\u0026gt; context) throws Exception { var event = APIGatewayV2HTTPEvent.builder().build(); handleRequest(event, null); } ... } Step 3: Implement Class priming Approach:\nThe class priming approach focuses on pre-loading required classes to achieve optimal performance. To implement class priming, generate the list of classes that are loaded during the application startup and function execution by running the application locally using the following JVM argument: -Xlog:class+load=info:classes-loaded.txt\nEnsure that your application classes included in the generated classes-loaded.txt file are not mutating state during static initialization. Note: the generated classes-loaded.txt contains class entries in the following format: [0.068s][info][class,load] software.amazon.awscdk.examples.unicorn.handler.ClassPriming source: file:/var/task/\nExtract only the fully qualified class names from each line and remove the additional logging information. For Example: software.amazon.awscdk.examples.unicorn.handler.ClassPriming\nUse the ClassLoaderUtil.loadClassesFromFile() utility method to extract the generated class entries.\n//src/main/java/software/amazon/awscdk/examples/unicorn/service/ClassLoaderUtil.java package software.amazon.awscdk.examples.unicorn; ... public class ClassLoaderUtil { ... public static void loadClassesFromFile() { log.info(\u0026#34;loadClassesFromFile-\u0026gt;started\u0026#34;); Path path = Paths.get(\u0026#34;classes-loaded.txt\u0026#34;); try (BufferedReader bufferedReader = Files.newBufferedReader(path)) { Stream\u0026lt;String\u0026gt; lines = bufferedReader.lines(); lines.forEach(line -\u0026gt; { var index1 = line.indexOf(\u0026#34;[class,load] \u0026#34;); var index2 = line.indexOf(\u0026#34; source: \u0026#34;); if (index1 \u0026lt; 0 || index2 \u0026lt; 0) { return; } var className = line.substring(index1 + 13, index2); try { Class.forName(className, true, ClassPriming.class.getClassLoader()); } catch (Throwable ignored) { } }); log.info(\u0026#34;loadClassesFromFile-\u0026gt;finished\u0026#34;); } catch (IOException exception) { log.error(\u0026#34;Error on newBufferedReader\u0026#34;, exception); } } ... } Read a file (such as, /classes-loaded.txt) that contains a list of classes that have been loaded during the application\u0026rsquo;s execution in the beforeCheckpoint() method.\nUse the Class.forName() method to load and initialize the class, ensuring that it is ready during the snapshot. Note: by systematically pre-loading these classes, the Class priming approach simplifies the optimization process and reduces the complexities associated with Invoke priming.\n//src/main/java/software/amazon/awscdk/examples/unicorn/handler/ClassPriming.java package software.amazon.awscdk.examples.unicorn.handler; ... import org.crac.Core; import org.crac.Resource; public class ClassPriming implements RequestHandler\u0026lt;APIGatewayV2HTTPEvent, APIGatewayV2HTTPResponse\u0026gt;, Resource { ... ConfigurableApplicationContext configurableApplicationContext = SpringApplication.run(UnicornApplication.class); this.unicornService = configurableApplicationContext.getBean(UnicornService.class); this.gson = configurableApplicationContext.getBean(Gson.class); Core.getGlobalContext().register(this); } @Override public APIGatewayV2HTTPResponse handleRequest(APIGatewayV2HTTPEvent event, Context context) { var unicorns = getUnicorns(); var body = gson.toJson(unicorns); return APIGatewayV2HTTPResponse.builder().withStatusCode(200).withBody(body).build(); } @Override public void beforeCheckpoint(org.crac.Context\u0026lt;? extends Resource\u0026gt; context) throws Exception { ClassLoaderUtil.loadClassesFromFile(); } ... } Step 4: AWS CDK Infrastructure Setup\nBefore proceeding, review the prerequisites in the project README file.\nThe CDK stack deploys a serverless application and required infrastructure for testing different Lambda optimization strategies. It creates a VPC with private subnets, an RDS for PostgreSQL instance with a database proxy, and five Lambda functions implementing different optimization approaches (ON_DEMAND without SnapStart, SnapStart without priming, SnapStart with invoke priming, and SnapStart with class priming). Each Lambda function is integrated with API Gateway for HTTP access, configured with Java 21 runtime on ARM64 architecture, and includes CloudWatch log groups for monitoring.\nFollow these steps to deploy the infrastructure:\nClone the sample repository: git clone https://github.com/aws-samples/lambda-priming-crac-java-cdk.git Deploy the CDK stack: cd lambda-priming-crac-java-cdk/infrastructure cdk synth cdk deploy --require-approval never --all 2\u0026gt;\u0026amp;1 | tee cdk_output.txt Save the API Gateway URLs: The deployment will output five URLs in this format: # ON_DEMAND endpoint (without SnapStart) LambdaPrimingCracJavaCdkStack.PrimingJavaRestApi1ONDEMANDEndpoint = https://[id].execute-api.us-east-1.amazonaws.com/prod/ # SnapStart without priming endpoint LambdaPrimingCracJavaCdkStack.PrimingJavaRestApi2SnapStartNOPRIMINGEndpoint = https://[id].execute-api.us-east-1.amazonaws.com/prod/ # SnapStart with invoke priming endpoint LambdaPrimingCracJavaCdkStack.PrimingJavaRestApi3SnapStartINVOKEPRIMINGEndpoint = https://[id].execute-api.us-east-1.amazonaws.com/prod/ # SnapStart with class priming endpoint LambdaPrimingCracJavaCdkStack.PrimingJavaRestApi4SnapStartCLASSPRIMINGEndpoint = https://[id].execute-api.us-east-1.amazonaws.com/prod/ # Database setup endpoint LambdaPrimingCracJavaCdkStack.PrimingJavaRestApi5DBLOADEREndpoint = https://[id].execute-api.us-east-1.amazonaws.com/prod/ Extract the URLs into variables for testing: ONDEMAND_URL=$(grep -oE \u0026#39;https://[a-zA-Z0-9.-]+\\.execute-api\\.[a-zA-Z0-9-]+\\.amazonaws\\.com/prod/\u0026#39; \u0026#34;cdk_output.txt\u0026#34; | head -n 1) NOPRIMING_URL=$(grep -oE \u0026#39;https://[a-zA-Z0-9.-]+\\.execute-api\\.[a-zA-Z0-9-]+\\.amazonaws\\.com/prod/\u0026#39; \u0026#34;cdk_output.txt\u0026#34; | head -n 2 | tail -n 1) INVOKEPRIMING_URL=$(grep -oE \u0026#39;https://[a-zA-Z0-9.-]+\\.execute-api\\.[a-zA-Z0-9-]+\\.amazonaws\\.com/prod/\u0026#39; \u0026#34;cdk_output.txt\u0026#34; | head -n 3 | tail -n 1) CLASSPRIMING_URL=$(grep -oE \u0026#39;https://[a-zA-Z0-9.-]+\\.execute-api\\.[a-zA-Z0-9-]+\\.amazonaws\\.com/prod/\u0026#39; \u0026#34;cdk_output.txt\u0026#34; | head -n 4 | tail -n 1) SETUP_URL=$(grep -oE \u0026#39;https://[a-zA-Z0-9.-]+\\.execute-api\\.[a-zA-Z0-9-]+\\.amazonaws\\.com/prod/\u0026#39; \u0026#34;cdk_output.txt\u0026#34; | head -n 5 | tail -n 1) Step 5: Load database and run performance testing using artillery:\nInitialize the database with sample data. curl -X GET \u0026#34;$SETUP_URL\u0026#34; #Expected output: {\u0026#34;message\u0026#34;:\u0026#34;Database schema initialized and data loaded\u0026#34;} Run performance tests for all endpoints artillery run -t \u0026#34;$ONDEMAND_URL\u0026#34; -v \u0026#39;{ \u0026#34;url\u0026#34;: \u0026#34;/unicorn\u0026#34; }\u0026#39; ./loadtest.yaml \u0026amp;\u0026amp; \\ artillery run -t \u0026#34;$NOPRIMING_URL\u0026#34; -v \u0026#39;{ \u0026#34;url\u0026#34;: \u0026#34;/unicorn\u0026#34; }\u0026#39; ./loadtest.yaml \u0026amp;\u0026amp; \\ artillery run -t \u0026#34;$INVOKEPRIMING_URL\u0026#34; -v \u0026#39;{ \u0026#34;url\u0026#34;: \u0026#34;/unicorn\u0026#34; }\u0026#39; ./loadtest.yaml \u0026amp;\u0026amp; \\ artillery run -t \u0026#34;$CLASSPRIMING_URL\u0026#34; -v \u0026#39;{ \u0026#34;url\u0026#34;: \u0026#34;/unicorn\u0026#34; }\u0026#39; ./loadtest.yaml Step 6: Compare the load test results for On-demand (non-SnapStart), SnapStart, Invoke priming, and Class priming\nThe performance test results in the table below are sorted from slowest to fastest startup latency. The function without SnapStart performs the slowest due to JVM initialization, class loading and JIT compilation that occurs when the function is invoked. Notice a 4.3x improvement with SnapStart, which resumes invocations from a pre-initialized snapshot thereby avoiding JVM initialization and initial JIT compilation. SnapStart with class priming achieves a 1.4x speed-up over SnapStart, by proactively loading/initializing classes during INIT so that they are included in your function\u0026rsquo;s snapshot. Finally, SnapStart with invoke priming achieves the fastest performance – with a 781.68ms p99.9 cold-start latency that is 1.8x faster than SnapStart. This is because in addition to initializing classes, it also executes methods on the instances of those classes, resulting in even more components being included in the function\u0026rsquo;s snapshot.\nNote that with invoke priming, any application code you execute must either be idempotent or modify stub data only. For instance, consider application code that triggers a financial transaction. If this code is executed during invoke priming with real user data, it may drive unintended effects with potentially serious consequences. Class priming avoids this, since application classes are initialized rather than being instantiated and their methods executed. This assumes that application code does not execute state modifying logic during class initialization. We recommend that you keep these considerations in mind when using invoke and/or class priming, and choose the appropriate approach for your use case.\nMethod Cold Start Invocations p50 P90 P99 p99.9 PrimingLogGroup-1_ON_DEMAND 128 5047.94 ms 5386.78 ms 6158.80 ms 6195.84 ms PrimingLogGroup-2_SnapStart_NO_PRIMING 111 1177.87 ms 1288.73 ms 1419.94 ms 1425.63 ms PrimingLogGroup-4_SnapStart_CLASS_PRIMING 82 857.81 ms 997.49 ms 1085.94 ms 1085.94 ms PrimingLogGroup-3_SnapStart_INVOKE_PRIMING 66 608.42 ms 688.88 ms 781.68 ms 781.68 ms Conclusion This post showed how AWS Lambda SnapStart, enhanced by CRaC runtime hooks, unlocks granular control over cold-start optimization for Java applications through two distinct priming strategies:\nInvoke Priming: improves performance by executing critical endpoints during snapshot creation, ideal for idempotent workflows. Class Priming: preloads classes without triggering business logic, mitigating side-effect risks. To implement these optimization techniques in your applications evaluate your use case and opt for the optimal priming approach. Track latency reductions and resource utilization of your application via Amazon CloudWatch metrics to quantify performance improvements. By integrating these strategies, developers can achieve sub-second cold starts while maintaining the scalability and cost-efficiency of serverless architecture using Java.\nTo dive deeper, check out the GitHub repository with the full example code, including setup instructions and reusable patterns you can adapt to your own projects. For more examples of Java applications running on AWS Lambda, visit serverlessland.com and explore a wide range of resources, tutorials, and real-world use cases.\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"Workshop Report: AWS Cloud Mastery Series #2 — DevOps on AWS Date: Monday, November 17, 2025\nTime: 8:30 AM – 5:00 PM\nLocation: Bitexco Financial Tower, Ho Chi Minh City\nHost: Kha Van\nAttendees: 316 participants\nOverview The second workshop in the AWS Cloud Mastery Series delivered a comprehensive, full-day deep dive into DevOps practices on AWS. The session spanned CI/CD automation, infrastructure-as-code, containerization, and monitoring, with live demonstrations and real-world case studies.\nMorning Session: Culture \u0026amp; CI/CD Pipeline (8:30 AM – 12:00 PM) 8:30 – 9:00 AM | Welcome \u0026amp; DevOps Mindset The session began with a recap of the previous AI/ML workshop and introduced DevOps culture and principles. Key metrics discussed:\nDORA Metrics: Deployment frequency, lead time, mean time to recovery (MTTR), change failure rate DevOps Benefits: Faster delivery, reduced risk, improved team collaboration 9:00 – 10:30 AM | AWS DevOps Services – CI/CD Pipeline A comprehensive walkthrough of the AWS CodePipeline ecosystem:\nSource Control AWS CodeCommit: Native Git repositories with IAM integration Git Strategies: GitFlow vs. Trunk-based development (tradeoffs and when to use each) Build \u0026amp; Test CodeBuild Configuration: Docker-based build environments, caching, artifact management Testing Pipelines: Unit tests, integration tests, and security scanning Deployment CodeDeploy Strategies: Blue/Green: Zero-downtime deployments with instant rollback Canary: Gradual rollout to a percentage of traffic Rolling: Sequential instance updates Target Instances: EC2, On-premises, Lambda Orchestration CodePipeline Automation: Connecting source → build → deploy stages Approval Gates: Manual review steps for production deployments Live Demo A full CI/CD pipeline walkthrough, showing:\nCode commit triggering CodeBuild Automated tests running and reporting Artifact storage in S3 CodeDeploy executing blue/green deployment Automatic rollback on failed health checks 10:30 – 10:45 AM | Break 10:45 AM – 12:00 PM | Infrastructure as Code (IaC) Two dominant IaC approaches on AWS were compared:\nAWS CloudFormation Templates \u0026amp; Stacks: JSON/YAML templates defining complete environments Drift Detection: Monitoring infrastructure changes outside CloudFormation Stack Policies: Preventing accidental updates to critical resources AWS CDK (Cloud Development Kit) Constructs: Reusable, composable infrastructure building blocks Language Support: TypeScript, Python, Java, .NET Higher Abstraction: Write infrastructure like application code Comparison \u0026amp; Selection CloudFormation: Lower-level control, JSON/YAML syntax, mature ecosystem CDK: Higher productivity, familiar programming languages, abstraction Live Demo Deploying infrastructure using both CloudFormation and CDK, highlighting:\nCloudFormation template structure and parameters CDK construct hierarchy and reuse Deployment timings and rollback behavior Afternoon Session: Containers \u0026amp; Observability (1:00 PM – 5:00 PM) 1:00 – 2:30 PM | Container Services on AWS Docker Fundamentals Microservices \u0026amp; Containerization: Breaking monoliths, independent scaling Image Layering: Optimizing image size and build cache Amazon ECR (Elastic Container Registry) Image Storage \u0026amp; Scanning: Vulnerability detection for container images Lifecycle Policies: Automatic cleanup of old or unused images IAM Integration: Fine-grained access to registries Amazon ECS vs. EKS ECS: Simpler orchestration, AWS-native, lower learning curve EKS: Kubernetes compatibility, multi-cloud options, richer ecosystem Deployment Strategies: Task placement, auto-scaling, service discovery AWS App Runner Simplified Container Deployment: No cluster management required Ideal for: Stateless web applications, APIs, microservices Automatic Scaling: Based on traffic or custom metrics Demo \u0026amp; Case Study A microservices deployment comparing:\nECS deployment with task definitions and services EKS deployment with pod manifests and operators App Runner deployment for a simple REST API Auto-scaling behavior under load 2:30 – 2:45 PM | Break 2:45 – 4:00 PM | Monitoring \u0026amp; Observability CloudWatch Metrics \u0026amp; Logs: Centralized collection from AWS services Alarms \u0026amp; Dashboards: Real-time visualization and alerting Log Insights: Query logs with SQL-like syntax AWS X-Ray Distributed Tracing: Track requests across microservices Service Map: Visual representation of application architecture Performance Insights: Identify bottlenecks and latency contributors Full-Stack Observability Setup Integrating CloudWatch, X-Ray, and application logs Correlation IDs for tracking requests end-to-end Setting appropriate alarm thresholds (avoiding alert fatigue) Best Practices Alerting: Alert on symptoms (latency, error rates) not just raw metrics Dashboards: Task-specific dashboards for oncall engineers On-Call Processes: Escalation policies, runbooks, incident postmortems Demo Building a complete observability solution:\nCloudWatch agent collecting metrics from EC2 instances Application logs flowing to CloudWatch Logs X-Ray tracing a multi-tier application Creating dashboards and alarms for key metrics 4:00 – 4:45 PM | DevOps Best Practices \u0026amp; Case Studies Deployment Strategies Feature Flags: Decoupling deployment from release A/B Testing: Experimentation with subsets of traffic Dark Launches: Running new infrastructure in parallel Automated Testing \u0026amp; CI/CD Integration Test Pyramid: Unit, integration, and end-to-end tests Test as Code: Keeping tests in version control Performance Testing: Catching regressions before production Incident Management Postmortems: Blameless reviews focusing on system improvements Runbooks: Documented procedures for common incidents On-Call Rotation: Fair distribution and escalation policies Real-World Case Studies Startup: From monolith to microservices, scaling from 10 to 1M users Enterprise: Legacy system modernization while maintaining 99.99% uptime 4:45 – 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up DevOps Career Pathways: AWS certification roadmap (Developer Associate, Solutions Architect Pro, DevOps Professional)\nPersonal Reflections What I Learned The breadth of the AWS DevOps ecosystem became clear—there\u0026rsquo;s a purpose-built service for each stage of the deployment pipeline. Rather than gluing together disparate tools, DevOps teams on AWS benefit from deep integration.\nThe contrast between ECS and EKS highlighted a key decision: simplified management vs. portability. For most use cases, ECS suffices; Kubernetes complexity pays off only when multi-cloud or complex orchestration is required.\nKey Insights Automation is non-negotiable: Manual deployments are error-prone and slow; CI/CD is table stakes. Observability beats monitoring: Recording what happened (logs/traces) is more valuable than just metrics. Infrastructure as Code is mandatory: Version-controlled, reviewable infrastructure reduces operational risk. DevOps is a culture, not a job title: Success requires collaboration between developers, ops, and security. Pick the right tool: ECS for simplicity, EKS for portability, App Runner for ease-of-use. Next Steps Set up a basic CI/CD pipeline using CodePipeline for my team\u0026rsquo;s projects Migrate an application to containers and deploy via ECS Implement CloudWatch dashboards and X-Ray tracing for existing services Design infrastructure templates using CDK for reproducible deployments Some pictures from the event This full-day workshop equipped me with practical DevOps knowledge and demonstrated that AWS provides end-to-end solutions for the entire deployment lifecycle. The emphasis on automation, observability, and culture resonates across all team sizes.\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/5-workshop/5.2-prerequiste/","title":"Prerequiste","tags":[],"description":"","content":"Create IAM User for this workshop In AWS Management Console, search and choose IAM Navigate to User, click Create user For User name, enter admin-user Check Provide user access to the AWS Management Console - optional For Console password, check Custom password Enter password for your user Uncheck Users must create a new password at next sign-in - Recommended for easy operation Click Next For Permissions options, check Attach policies directly Click Create policy You will be directed to Create policy page For Policy editor, switch to JSON Copy and paste this policy { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;InfrastructureManagement\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;iam:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;BackendComputeAndAPI\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:*\u0026#34;, \u0026#34;apigateway:*\u0026#34;, \u0026#34;execute-api:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;DatabaseAndAuth\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:*\u0026#34;, \u0026#34;cognito-idp:*\u0026#34;, \u0026#34;cognito-identity:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;IoTServices\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;iot:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;StorageAndHosting\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:*\u0026#34;, \u0026#34;cloudfront:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;MonitoringAndLogging\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;events:*\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Click Next For Policy name, enter your policy name (E.g. SmartOfficeAdminFullAcccess) Add Tags for cost and operation management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) Click Create policy Go back to Step 2 Set permissions of Create user Search and choose your policy name Click Next Add Tags for cost and operation management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) Click Create user Login with your User account to begin this workhop "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Smart Office Management System for Lab Research A Unified AWS Serverless Solution for Real-Time Monitoring \u0026amp; Administration 1. Executive Summary The Smart Office Management System is proposed by Team Skyscraper from FPTU HCM Campus, inspired by the operational excellence observed during a field trip to the AWS office in Ho Chi Minh City. Traditional office management lacks real-time visibility into room conditions (temperature, humidity, light) and relies heavily on manual oversight. To address this, we propose a centralized Management Console built on a fully AWS Serverless architecture. By leveraging services like AWS IoT Core, Lambda, and DynamoDB, the system collects sensor data every 2-5 minutes to support real-time monitoring and allows administrators to manage device configurations remotely. This project also serves as a strategic \u0026ldquo;First Cloud AI Journey\u0026rdquo;, enabling the team to bridge the gap between theoretical knowledge and practical application of Cloud Computing.\n2. Problem Statement What’s the Problem? Nowadays, managing office environments in research labs requires manual intervention to check device statuses (lights, air conditioners) and environmental conditions. Managers often lack the data needed to make informed decisions about energy usage or room comfort. Operating devices on fixed schedules (e.g., 8 a.m. to 5 p.m.) without regarding actual room usage or environmental factors leads to energy waste. Furthermore, without a centralized dashboard, administrators cannot quickly detect anomalies or configure settings for multiple rooms efficiently.\nThe Solution The platform uses AWS IoT Core to ingest MQTT data from room sensors, AWS Lambda and API Gateway for backend logic and processing, Amazon DynamoDB for storing sensor logs and room configurations, and Amazon S3 combined with CloudFront to host the web management dashboard. Access is strictly secured via Amazon Cognito. Amazon EventBridge is utilized to handle scheduled automation tasks, while Amazon SNS ensures timely notifications for system alerts. This solution replaces manual tracking with a digital, real-time management console capable of monitoring multiple rooms simultaneously.\nBenefits and Return on Investment The Smart Office Management System enhances operational efficiency by providing a single pane of glass for monitoring and configuration. It empowers lab managers to control devices remotely and make data-driven decisions. Beyond operational improvements, the project provides a reusable serverless foundation for future IoT research at the university.\nMonthly operating costs are estimated at $1.81 USD, utilizing the AWS Free Tier for services like Lambda, API Gateway, and DynamoDB. Major costs include CloudFront ($1.27) and CloudWatch ($0.25), totaling approximately $21.72 USD per year. Since the system leverages existing ESP32 hardware and sensors, there are no additional capital expenditures. The system offers immediate value through time savings and reduced management effort.\n3. Solution Architecture The Smart Office system adopts a fully serverless AWS architecture optimized for cost efficiency and scalability. Data from multiple sensor hubs is transmitted to AWS IoT Core, processed by Lambda functions, and stored in DynamoDB for real-time monitoring and configuration management. EventBridge automates scheduled device actions, while SNS handles system notifications. The web dashboard is hosted on S3 and delivered securely via CloudFront, with user authentication managed through Amazon Cognito. This architecture minimizes operational overhead and ensures high reliability for smart environment control.\nAWS Services Used AWS IoT Core: Ingests and manages MQTT data from smart room hubs, enabling secure communication between edge devices and the cloud. AWS Lambda: Executes backend logic for processing sensor telemetry, handling API requests, and executing management commands (Serverless Compute). Amazon API Gateway: Exposes secure RESTful endpoints for the web dashboard to interact with the backend services. Amazon DynamoDB: Provides fast, NoSQL storage for room configurations, device states, and historical sensor logs. Amazon EventBridge: Orchestrates event-driven workflows, such as scheduled configuration updates or heartbeat checks. Amazon SNS: Sends email notifications to administrators regarding system alerts or critical updates. Amazon S3: Hosts the frontend static assets (HTML, CSS, JS) for the Management Dashboard. Amazon CloudFront: Delivers the web application globally with low latency and SSL security. Amazon Cognito: Manages user identity, authentication, and access control for the Management Console. Amazon CloudWatch: Collects logs and metrics to monitor system health and debug Lambda executions. Component Design Sensor Hubs: IoT-enabled devices (ESP32) in each room collect telemetry (temperature, humidity, light) and transmit it to AWS IoT Core every few minutes. Data Ingestion: AWS IoT Core rules trigger the HandleTelemetry Lambda, which validates data and persists it to Amazon DynamoDB. Configuration Management: Administrators use the dashboard to update room settings. The RoomConfigHandler Lambda updates DynamoDB and pushes changes to devices via IoT Core Shadows or MQTT. User Interaction: The Web Dashboard (on S3/CloudFront) visualizes real-time data and provides a control interface. User Authentication: Amazon Cognito ensures only authorized lab members can log in and access sensitive room data. Monitoring \u0026amp; Reliability: Amazon CloudWatch tracks system performance, ensuring high availability and rapid troubleshooting. 4. Technical Implementation Implementation Phases\nResearch \u0026amp; Foundation (Weeks 1-7): Study core AWS services (IoT Core, Lambda, DynamoDB, S3, API Gateway, Cognito) and understand Serverless design patterns. Architecture Design \u0026amp; Estimation (Week 8): Finalize the solution diagram for an 8-room setup and use the AWS Pricing Calculator to forecast the budget. Development (Weeks 9-12): Implement firmware/scripts for IoT data simulation. Develop Backend: Lambda functions, DynamoDB tables, and API Gateway resources using CloudFormation/CDK. Develop Frontend: Build the Management Dashboard and integrate with APIs. Testing \u0026amp; Deployment (Week 13): Perform end-to-end testing, validate data flow from sensors to the dashboard, and deploy the system to the production environment. Technical Requirements\nHardware Layer: ESP32-based Sensor Hubs monitoring environmental metrics. Cloud Layer: A fully serverless stack on AWS (IoT Core, Lambda, DynamoDB, API Gateway, S3, CloudFront, Cognito, EventBridge, SNS). DevOps: Infrastructure as Code (IaC) using AWS CloudFormation for reproducible deployments. Interface: A responsive web dashboard allowing real-time monitoring and configuration updates. 5. Timeline \u0026amp; Milestones Project Timeline\nWeeks 1–7: Deep dive into AWS services and complete \u0026ldquo;First Cloud AI Journey\u0026rdquo; fundamental training. Week 8: Design the system architecture and finalize cost estimation. Weeks 9–12: Core development phase (Backend logic, Database schema, Frontend UI integration). Week 13: System integration testing, debugging, and final Go-Live presentation. 6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator.\nOr you can download the Budget Estimation File.\nInfrastructure Costs AWS Services:\nAmazon DynamoDB: Free (Always Free Tier: 25GB Storage). AWS Lambda: Free (Always Free Tier: 1M requests/month). AWS IoT Core: $0.18/month (8 devices, sending data every 2 mins). Amazon API Gateway: Free (Free Tier: 1M calls/month for 12 months). Amazon S3: Free (Standard storage \u0026lt; 5GB). Amazon CloudFront: $1.27/month (Based on est. data transfer and requests). Amazon EventBridge: Free (Free Tier events). Amazon SNS: $0.02/month (Email notifications). Amazon CloudWatch: $0.25/month (Log ingestion and storage). Amazon Cognito: Free (Free Tier: 50,000 MAUs). Hardware: Mock script Total: ≈ $1.81/month, or $21.72/year (optimized within AWS Free Tier).\n7. Risk Assessment Risk Matrix IoT Connectivity Issues: Medium impact, medium probability. Sensor Data Inaccuracy: Medium impact, low probability. Unexpected AWS Charges: Low impact, low probability (due to strict budget alerts). Security Misconfiguration: High impact, low probability. Mitigation Strategies Connectivity: Implement retry logic on edge devices and local buffering. Cost: Configure AWS Budgets to alert when spending exceeds $5.00. Security: Enforce strict IAM policies (Least Privilege) and require authentication for all API access via Cognito. Reliability: Use CloudWatch Logs to trace errors in Lambda execution immediately. Contingency Plans Enable manual override controls if the cloud system becomes unavailable. Maintain a backup of the CloudFormation templates for rapid redeployment. 8. Expected Outcomes Technical Improvements: Replaces manual checks with real-time digital monitoring. Provides a centralized platform for managing configurations across multiple rooms. Establishes a scalable architecture capable of supporting more devices in the future. Long-term Value: Serves as a practical learning hub for students to master AWS Serverless technologies. Provides data insights that can lead to better energy usage policies in the lab. Demonstrates a cost-effective, production-ready cloud solution. Proposal Link Smart_Office_Proposal\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Master basic Linux command line skills needed for EC2 and IoT devices. Learn Python basics for Lambda functions and scripting. Understand AWS Identity and Access Management (IAM) fundamentals. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory videos on Linux Basics - Practice: + Connect to EC2 instance via SSH + Practice file navigation (ls, cd, pwd) and permissions (chmod, chown) 09/15/2025 09/15/2025 https://www.udemy.com/course/linux-mastery/ 3 - Watch theory videos on Python for Beginners - Practice: + Write a script to print \u0026ldquo;Hello World\u0026rdquo; + Create a simple calculator script using functions 09/16/2025 09/16/2025 https://www.coursera.org/programs/fptu-fall-2025-zmahp/specializations/python 4 - Watch videos of Module 5 (IAM) - Practice: + Create IAM Users and Groups + Create a custom IAM Policy 09/17/2025 09/17/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 5 - Watch theory videos on JSON Fundamentals (Needed for IAM Policies) - Practice: + Write a JSON object representing a Smart Lightbulb + Validate JSON syntax online 09/18/2025 09/18/2025 https://www.w3schools.com/js/js_json_intro.asp 6 - Review IAM Roles concepts -Learn basic about Amazon RDS\n- Practice: + Create an IAM Role for an EC2 instance to access S3 (Lab 5) +Create RDS database instance 09/19/2025 09/19/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000005.awsstudygroup.com/ Week 2 Achievements: Navigated Linux file system and managed permissions on EC2. Wrote basic Python scripts using variables and functions. Created secure IAM Users, Groups, and custom JSON Policies. Attached IAM Roles to EC2 instances for secure access. "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":"Introducing Just-in-time node access using AWS Systems Manager by Chetan Makvana, Anthony Verleysen, and Mark Brealey | on 29 APR 2025 | in Announcements, AWS Systems Manager, Management Tools, Security | Permalink | Share\nToday, we\u0026rsquo;re excited to announce the general availability of just-in-time node access, a new capability in AWS Systems Manager. Just-in-time node access enables dynamic, time-bound access to Amazon Elastic Compute Cloud (Amazon EC2), on-premises, and multicloud nodes managed by AWS Systems Manager. It uses a policy-based approval process, allowing you to remove long-standing access while maintaining operational efficiency and enhancing security.\nOrganizations expanding their operations to thousands of nodes require identity driven granular permissions to support their audit and compliance objectives. They want to eliminate long term credentials entirely. The practice of using long-term credentials for node access creates security vulnerabilities, increasing the risk of unauthorized access and potential breaches.\nPreviously, customers faced a challenging trade-off between security and operational efficiency. Rather than carefully determining who needed access to specific resources, IT teams would grant excessive permissions to large groups of users. This practice created increased risk of accidental operator errors, and opportunity for bad actors, driven by the need for operational convenience. They either maintained long term credentials, which increased risk of compromised security, or implemented restrictive access controls that slowed incident response. Custom-built solutions proved complex to maintain and scale; whereas non-AWS tools using agents require identity and permissions to access nodes.\nOverview Just-in-time node access helps you implement least-privilege access while ensuring operational teams can quickly respond to issues. It works seamlessly across your AWS Organization, allowing you to set up consistent access controls whether you\u0026rsquo;re managing a single account or multiple accounts. This new capability allows administrators to define precise access controls through approval policies that specify who can access which nodes and under what conditions. Organizations can choose between manual approval processes with multiple approvers or condition-based auto-approval policies, providing flexibility to match their security requirements.\nFor example, administrators can establish auto-approval policy to quickly provide on-call engineers access during incidents, granting access only to operators in an on-call AWS IAM Identity Center group. Through just-in-time node access, operators can request access to nodes when they need it. Based on pre-configured approval policies, they receive temporary access that automatically expire after a defined time window. Upon approval, they can directly access these nodes via a one-click browser-based shell, AWS Command Line Interface (AWS CLI) or Remote Desktop Protocol (RDP) supported by Systems Manager, without the need to open inbound ports or manage SSH keys.\nTo simplify the approval process, just-in-time node access integrates with tools like Slack and Microsoft Teams through Amazon Q Developer, and email to notify approvers of pending requests. Systems Manager also emits events to Amazon EventBridge for status updates to just-in-time node session access request. These events can be routed to Amazon Simple Notification Service (Amazon SNS) for notifications or integrated with your internal systems, allowing your teams to track and respond to access requests through your existing workflows. This enables you to monitor access requests and maintain audit trails across your organization. Furthermore, just-in-time node access can provide additional visibility into operator activities by logging commands run during sessions and recording their actions during RDP sessions.\nSystems Manager offers a free trial of just-in-time node access per account per Region, allowing you to fully explore and evaluate the feature for your organization. This trial period includes the remainder of the billing period in which you enable the feature, plus the entire next billing period. During this trial period, you\u0026rsquo;ll have access to all capabilities, enabling you to test configurations and access policies without any additional charges. After the trial concludes, just-in-time node access becomes a paid service, with charges based on your usage patterns. For detailed pricing information and cost breakdowns, please refer to AWS Systems Manager Pricing.\nUsing Just-in-Time Node Access When you implement just-in-time node access, you\u0026rsquo;ll work with three distinct roles: Administrator, Operator, and Approver. Administrator establishes and maintains approval policies. Operator initiates access requests for specific nodes. And approver reviews and authorizes access requests.\nLet\u0026rsquo;s walk through how you can set up and use this feature, using a scenario where your on-call engineer needs access to a production system, specifically to an instance named \u0026lsquo;r2d2-app-01\u0026rsquo; from the below fleet of instances as shown in figure 1.\nFigure 1: Amazon EC2 console showing list of EC2 instances\nWe will explore how an on-call engineer (Operator) can request access to production system, with the DevOps lead (Approver) managing the approval, all within the approval policy defined by the Administrator.\nSetting up Just-in-time node access as an Administrator Step 1 – Enabling Just-in-Time Node Access In this walk-through, we are going to enable just-in-time node access for the AWS Organization. To get started, you must first set up the Systems Manager unified console. Once the unified console is setup, you can then enable just-in-time node access in Systems Manager.\nYou can then choose which Organization Units (OUs) and AWS Regions to target for deployment. This lets you precisely control where the solution is implemented, whether across your entire organization or in specific areas as shown in figure 2.\nFigure 2: Enable just-in-time node access\nStep 2 – Creating approval policies After enabling the feature, the next crucial step is creating approval policies. Approval policies determine how users gain access to nodes. These policies come in three types: auto-approval, manual approval, and deny-access policies. Auto-approval policy defines which nodes users can connect to automatically. Manual approval policy defines the number and levels of manual approvals that must be provided to access the nodes you specify. Deny-access policy explicitly prevents the auto-approval of access requests to the nodes you specify.\nIn our example, we will focus on creating a manual approval policy for nodes tagged with Workload:Application01, which includes our \u0026lsquo;r2d2-app-01\u0026rsquo; node.\nTo create the policy, navigate to the AWS Systems Manager console, choose just-in-time node access in the navigation pane, select the Approval policies tab, and choose Create manual policy. The policy configuration requires several key components.\nFirst, in the Approval policy details section, provide a name and description for the approval policy, along with setting the maximum access duration as shown in figure 3. This duration determines how long approved access remains valid before automatically expiring.\nFigure 3: Manual approval policy page\nIn the Targets section, use tag key-value pairs to define which nodes the policy applies to. For this example, we\u0026rsquo;ll target nodes tagged with Workload:Application01, which includes our \u0026lsquo;r2d2-app-01\u0026rsquo; node. This approach ensures the policy applies to all nodes associated with Application01 as shown in figure 4.\nFigure 4: Manual approval policy targets\nIn the Access request approvers section, you\u0026rsquo;ll designate individuals or groups authorized to approve access requests. In our scenario, we\u0026rsquo;ll assign the DevOps lead role as the approver. Access requests approvers can be IAM Identity Center users and groups or IAM users, groups, and roles as shown in figure 5.\nFigure 5: Access requests approval\nYou can also define automated access rules using the Cedar policy language, eliminating the need for manual approvals in trusted scenarios. Think of auto-approval policies as your organization\u0026rsquo;s pre-approved access rulebook. These policies specify which nodes users can access automatically, based on predefined conditions and trust levels. For more information, see Create an auto-approval policy for just-in-time node access and Statement structure and built-in operators for auto-approval and deny-access policies.\nFor example, you can create an auto-approval policy that automatically allows members of the \u0026ldquo;DevOpsTeam\u0026rdquo; group to access nodes tagged with Environment: Development using the following Cedar policy:\n// Policy to permit access to Development nodes for members of the DevOpsTeam IDC group permit ( principal in AWS::IdentityStore::Group::\u0026#34;911b8590-7041-70fa-d20b-12345EXAMPLE\u0026#34;, action == AWS::SSM::Action::\u0026#34;getTokenForInstanceAccess\u0026#34;, resource) when { resource.hasTag(\u0026#34;Environment\u0026#34;) \u0026amp;\u0026amp; resource.getTag(\u0026#34;Environment\u0026#34;) == \u0026#34;Development\u0026#34; }; Requesting access as an Operator When you need to access a protected node as an operator, you\u0026rsquo;ll see a streamlined request process. Instead of immediate access, you\u0026rsquo;ll be prompted to submit an access request when attempting to connect through Session Manager. You\u0026rsquo;ll need to provide a justification for access as shown in figure 6.\nFigure 6: Operator raising a request to access the node\nAfter submitting your request, you can monitor its status through the Access Requests tab as shown in figure 7. You\u0026rsquo;ll be able to track your request through the approval process and know exactly when your access becomes available. You\u0026rsquo;ll receive notifications via your preferred communication channel, whether that\u0026rsquo;s email, Slack, Microsoft Teams, or another integrated platform. For more information, see Configure notifications for just-in-time access requests.\nFigure 7: List of access request page\nManaging approvals As an approver, you\u0026rsquo;ll receive notifications of pending access requests through your configured notification channel. You can programmatically approve requests using the AWS Command Line Interface (AWS CLI), or your preferred SDK. Or you can review these requests in the Systems Manager console under the Requests for me tab as shown in figure 8.\nFigure 8: List of access requests pending for approval\nAfter reviewing the request, you can either approve or reject the request and optionally add a comment related to the decision.\nCompleting the access cycle Once request is approved, as an operator, you receive notification that your access has been granted. You can then connect to the node using the AWS Management console or AWS CLI for the duration in the approval policy as shown in figure 9.\nFigure 9: Operator accessing the managed node\nConclusion In this blog, we introduced just-in-time node access, a new capability in AWS Systems Manager. Just-in-time node access solves the challenge of balancing operational efficiency with security requirements by eliminating standing privileges while ensuring swift access to Amazon EC2, on-premises, and multicloud nodes. Through its flexible policy-based approach, and support for both manual and automatic approvals, you can now implement zero standing privileges without compromising operational capabilities.\nSystems Manager offers a free trial of just-in-time node access, allowing you to fully explore and evaluate the feature for your organization.\nTo learn more, see Just-in-time node access using Systems Manager for more details.\nCheck out this interactive demo for a full visual tour of the just-in-time node access experience.\nAbout the authors Chetan Makvana Chetan Makvana is an Enterprise Solutions Architect at Amazon Web Services. He helps enterprise customers in designing scalable, resilient, secured and cost effective enterprise grade solution using AWS services. He is a technology enthusiast and a builder with a core area of interest on generative AI, serverless, app modernization and DevOps. Outside of work, he enjoys binge-watching, traveling and music.\nAnthony Verleysen Anthony Verleysen is a Senior Product Manager - Technical within the AWS Systems Manager team. He is the the product manager for Patch Manager and Distributor. Outside of work, Anthony is an avid soccer and tennis player.\nMark Brealey Mark Brealey is a Senior Migration Solutions Architect, he empowers partners to build robust, secure, and efficient cloud architectures. He specializes in designing scalable solutions that help organizations maximize their AWS infrastructure while ensuring operational excellence.\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"Workshop Report: AWS Cloud Mastery Series #3 — Well-Architected Security Pillar Date: Friday, November 29, 2025\nTime: 8:30 AM – 12:00 PM\nLocation: Bitexco Financial Tower, Ho Chi Minh City\nHost: Kha Van\nAttendees: 355+ participants\nOverview The third and final workshop in the AWS Cloud Mastery Series focused on cloud security through the lens of the AWS Well-Architected Framework\u0026rsquo;s Security Pillar. The half-day session covered five key pillars: IAM, Detection, Infrastructure Protection, Data Protection, and Incident Response.\nSession Breakdown 8:30 – 8:50 AM | Opening \u0026amp; Security Foundations The session began with an overview of the Security Pillar\u0026rsquo;s role in well-architected design:\nCore Principles:\nLeast Privilege: Grant only necessary permissions Zero Trust: Never trust, always verify Defense in Depth: Multiple layers of security controls Shared Responsibility Model: AWS manages infrastructure security; customers manage application and data security\nTop Threats in Vietnam: Ransomware, credential compromise, exposed databases, insecure APIs\n8:50 – 9:30 AM | Pillar 1: Identity \u0026amp; Access Management Modern IAM Architecture Users, Roles, Policies: Fundamentals of AWS permission model Avoiding Long-Term Credentials: Service roles and temporary credentials AWS IAM Identity Center: Enterprise SSO and permission sets for multi-account deployments Access Control at Scale Service Control Policies (SCP): Organization-wide guardrails preventing dangerous actions Permission Boundaries: Limiting maximum permissions for IAM users and roles MFA Options: TOTP (Time-based One-Time Password) vs. FIDO2 (hardware keys) Continuous Validation Access Analyzer: Identifying overly permissive policies and external access Credential Rotation: Automating key rotation for programmatic access Session Duration Limits: Requiring re-authentication for sensitive operations Mini Demo: IAM Policy Validation Creating a policy and simulating access Identifying permission gaps Using Access Analyzer to detect public access 9:30 – 9:55 AM | Pillar 2: Detection \u0026amp; Continuous Monitoring Logging Infrastructure CloudTrail (Organization-level): All API calls across AWS accounts GuardDuty: Managed threat detection using machine learning Security Hub: Centralized security findings and compliance status Logging at Every Layer VPC Flow Logs: Network traffic analysis (accepted and rejected connections) ALB/Application Logs: Request-level visibility S3 Access Logs: Object-level access tracking Database Audit Logs: Query and user activity Alerting \u0026amp; Automation EventBridge: Route security events to Lambda, SNS, or SQS for automated response CloudWatch Alarms: Triggering alerts on specific log patterns Detection-as-Code: Version-controlling detection rules alongside infrastructure 9:55 – 10:10 AM | Coffee Break 10:10 – 10:40 AM | Pillar 3: Infrastructure Protection Network Security VPC Segmentation: Private subnets for databases, public for load balancers Security Groups vs. NACLs: Stateful vs. stateless filtering Application Models: Tiered architecture with network isolation Advanced Protection AWS WAF (Web Application Firewall): Protecting web applications from common attacks AWS Shield: DDoS protection (Standard automatic, Advanced for high-value applications) AWS Network Firewall: Centralized firewall for VPCs Workload Security EC2 Security: Security groups, Systems Manager Session Manager (no SSH keys) ECS/EKS Security: Pod security policies, network policies, container scanning Secrets in Transit: mTLS, encrypted API endpoints 10:40 – 11:10 AM | Pillar 4: Data Protection Encryption Strategy At-Rest Encryption:\nS3: SSE-KMS with customer-managed keys RDS: Transparent Data Encryption (TDE) EBS: Encrypted volumes with custom KMS keys DynamoDB: Encryption with AWS KMS In-Transit Encryption: HTTPS/TLS for all data movement\nKey Management AWS KMS: Key policies, grants, automatic rotation Separation of Duties: Key administrator vs. key user roles Key Rotation Strategy: Balancing security with operational complexity Secret Management AWS Secrets Manager: Database credentials, API keys, certificates Parameter Store: Configuration parameters (encrypted with KMS) Rotation Patterns: Lambda-based automatic rotation Access Control: IAM policies restricting which applications can access secrets Data Classification PII/Sensitive Data Identification: Macie for automatic discovery Access Guardrails: Restricting access to sensitive data stores Audit Trails: Tracking who accessed what and when 11:10 – 11:40 AM | Pillar 5: Incident Response IR Lifecycle (per AWS) Preparation: Tooling, runbooks, team training Detection \u0026amp; Analysis: Identifying and characterizing incidents Containment: Stopping the attack and limiting damage Eradication: Removing the attacker\u0026rsquo;s presence Recovery: Restoring systems to normal operations Post-Incident Review: Learning from the incident Incident Playbooks Scenario 1: Compromised IAM Key\nDetection: GuardDuty alerts on unusual API calls Containment: Disable compromised credentials immediately Eradication: Rotate access keys, review CloudTrail for unauthorized actions Recovery: Verify no backdoors remain Scenario 2: S3 Bucket Publicly Exposed\nDetection: CloudTrail shows bucket policy modified or object ACL changed Containment: Restrict public access immediately Eradication: Review who modified policies (audit trail) Recovery: Verify no sensitive data was accessed Scenario 3: EC2 Malware Detection\nDetection: GuardDuty identifies suspicious EC2 behavior Containment: Isolate instance in security group, snapshot for forensics Eradication: Update OS/patch, rotate credentials Recovery: Redeploy clean instance from golden image Automation \u0026amp; Tooling Snapshot \u0026amp; Isolation: Automatic snapshot creation and network isolation Evidence Collection: Preserving logs and data for post-mortem Auto-Response: Lambda functions triggering remediation (disable user, isolate instance) Notification: SNS alerts to security team and incident commander 11:40 – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A Key Takeaways The five pillars are interconnected: strong IAM is useless without detection Vietnamese enterprises face real threats; preparation is essential AWS provides end-to-end tools; the barrier is process and culture Learning Roadmap AWS Security Fundamentals: Introduction level AWS Certified Security – Specialty: Deep technical knowledge AWS Solutions Architect Professional: Architectural decision-making Hands-on Practice: CloudLabs, building test scenarios Common Pitfalls in Vietnam Over-reliance on perimeter security (assumes internal trust) Storing secrets in code or configuration files Neglecting log retention and compliance requirements Manual incident response processes (slow and error-prone) Personal Reflections What I Learned Security is not a single control—it\u0026rsquo;s a system of overlapping defenses. AWS provides comprehensive tools, but success requires process discipline and cultural commitment to least privilege.\nThe incident response emphasis resonated strongly. Too many organizations prepare for zero-day exploits but fail to respond to compromised credentials (the most common attack vector). Well-documented playbooks and automated response are force multipliers.\nKey Insights Detection \u0026gt; Prevention: Assume breach mentality—focus on detecting compromise quickly. Automation is critical: Manual incident response is too slow; Lambda/Step Functions should handle routine responses. Logging is mandatory: Without comprehensive logs, you cannot investigate or audit effectively. IAM is the foundation: Every other control depends on solid identity and access management. Culture matters: Technical controls fail without security-conscious teams and processes. Next Steps Audit current IAM permissions and identify over-privilege cases Enable GuardDuty and Security Hub for continuous threat detection Build incident response playbooks specific to our infrastructure Set up automated remediation for common security findings Implement encryption at-rest for all databases and S3 buckets This half-day security workshop provided a strategic overview of the AWS Well-Architected Security Pillar and practical incident response frameworks. The emphasis on automation, detection, and incident playbooks will directly improve our organization\u0026rsquo;s security posture.\nScale least-privilege access enforcement using AWS organizational policies and permission boundaries across multiple accounts Leverage AWS services (EventBridge, SNS, SQS) to build automated security alerting and response workflows Codify security detection rules and store them in version control to enable collaborative incident response Implementation Strategy Roll out MFA across all accounts and implement fine-grained least-privilege policies for all roles Deploy automated event routing and alerting to reduce security response time Version security rules as infrastructure code and use pull requests for changes to detection logic Takeaway The security pillar sessions significantly strengthened my practical abilities in designing alerting systems, building automation, and responding to incidents. The presenters shared lessons from large-scale cloud deployments, highlighting common mistakes and proven remediation patterns.\nThe workshop expanded my grasp of cloud security operations and incident response procedures; I now have concrete steps to enhance alerting pipelines, operational automation, and policy controls.\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/5-workshop/5.3-run-cloudformation-stack/","title":"Set up cloudformation","tags":[],"description":"","content":"Download resources Download these CloudFormation template files:\nsmart_office_budget.yaml smart_office_s3_cloudfront.yaml smart_office_cognito.yaml smart_office_dynamodb.yaml smart_office_lambda_authenticate_with_dynamodb_cognito.yaml smart_office_lambda_readonly_with_dynamodb.yaml smart_office_lambda_crud_with_dynamodb_cognito.yaml smart_office_lambda_crud_with_dynamodb_iot.yaml smart_office_iot_core.yaml smart_office_api_gateway.yaml Deploy CloudFormation Stacks In AWS Management Console, search and choose CloudFormation Click Create stack For Prepare template, check Choose an existing template For Template source, check Upload a template file Click Choose file Choose file smart_office_budget.yaml Click Next For Stack name, enter SmartOffice-Budget-Dev Click Next Add Tags for cost and operation management (Key: Project, Value: SmartOffice; Key: Environment, Value: Dev) For Stack failure options, check Preserve successfully provisioned resources (To keep created resource for debugging) Click Next Check again and click Submit Do the same for other files with exactly name Template name Stack name smart_office_s3_cloudfront.yaml SmartOffice-S3-CloudFront-Dev smart_office_cognito.yaml SmartOffice-Cognito-Dev smart_office_dynamodb.yaml SmartOffice-DynamoDB-Dev smart_office_lambda_authenticate_with_dynamodb_cognito.yaml SmartOffice-Authenticate-Lambda-Dev smart_office_lambda_readonly_with_dynamodb.yaml SmartOffice-ReadOnly-Lambda-Dev smart_office_lambda_crud_with_dynamodb_cognito.yaml SmartOffice-Crud-Lambda-Dev smart_office_lambda_crud_with_dynamodb_iot.yaml SmartOffice-IoT-Lambda-Dev smart_office_iot_core.yaml SmartOffice-IoT-Core-Dev smart_office_api_gateway.yaml SmartOffice-API-Gateway-Dev "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"Overview of Translated Technical Blogs This section contains curated translations of three AWS technical blog posts covering advanced topics in cloud computing, performance optimization, and security. Each blog provides practical insights and real-world implementations relevant to modern cloud architecture.\nBlog 1 - Risk Management Analytics: Turning Uncertainty into Opportunity This comprehensive blog explores how financial services organizations are transforming their risk management capabilities through digital innovation on AWS. The article discusses how sophisticated enterprises are moving beyond reactive compliance approaches to leverage analytics-driven methodologies.\nKey Topics Covered:\nNavigating regulatory complexity in financial services with adaptive governance frameworks Real-world case studies demonstrating customer success: Toyota Financial Services Italia: Implemented SAS Viya to centralize fragmented customer data, enabling predictive customer churn models and achieving improved personalization Global Financial Services Company: Consolidated compliance operations with Smarsh Enterprise Platform, achieving $7 million in annual cost savings and 70% reduction in false positives Fortitude Re: Built comprehensive risk management with Numerix, reducing intraday hedging mismatches through cloud-native infrastructure Cloud-native solutions eliminating infrastructure overhead while maintaining security standards Strategic insights from practitioners on executive support, compliance balance, and risk value demonstration Takeaway: AWS Marketplace provides access to proven risk management solutions that enable financial institutions to transform compliance from a cost center into a competitive advantage through centralized data analytics and real-time insights.\nBlog 2 - Optimizing AWS Lambda Cold Start Performance using SnapStart and Priming Strategies This technical deep dive explores advanced performance optimization techniques for AWS Lambda functions, particularly for latency-sensitive serverless applications. The blog demonstrates how to achieve sub-second startup times for Spring Boot applications running on Lambda.\nKey Topics Covered:\nSnapStart Technology: Lambda feature introduced at re:Invent 2022 that reduces cold start latency from several seconds to sub-second by snapshotting initialized execution environments Priming Techniques for further optimization: Invoke Priming: Pre-execute code paths during INIT phase to ensure JIT compilation and class loading occur before snapshot Class Priming: Proactively load classes using Java\u0026rsquo;s forName() method without executing methods, providing safer alternative for state-sensitive applications Real Performance Results: Sample Spring Boot application achieved 4.3x reduction in cold start time (6.1s → 1.4s) with SnapStart, with additional gains possible through priming Implementation Walkthrough: Step-by-step guide using Spring Boot 3, CRaC runtime hooks, RDS Proxy connectivity, and AWS Lambda environments Best Practices: Ensuring priming operations are idempotent and don\u0026rsquo;t modify application state Takeaway: Combining SnapStart with strategic priming techniques enables development teams to build highly responsive, production-grade serverless applications that meet strict latency requirements without significant code refactoring.\nBlog 3 - Introducing Just-in-Time Node Access using AWS Systems Manager This announcement blog introduces a game-changing security feature for managing access to EC2, on-premises, and multicloud infrastructure. Just-in-time (JIT) node access eliminates the need for long-standing credentials while enabling rapid incident response.\nKey Topics Covered:\nSecurity Challenge Addressed: Long-term credentials create vulnerabilities; traditional approaches force trade-offs between security and operational efficiency JIT Node Access Solution: Policy-based dynamic, time-bound access across AWS Organizations with: Three Role Model: Administrator (policy management), Operator (access requests), Approver (authorization) Flexible Approval Options: Manual multi-level approvals or Cedar policy language for condition-based auto-approval Integration Capabilities: Slack, Microsoft Teams (via Amazon Q Developer), email notifications, and EventBridge for audit trail integration Practical Implementation: Setting up unified Systems Manager console for organization-wide control Creating approval policies targeting specific node tags (e.g., Workload:Application01) Operators submitting justified access requests with automatic expiration One-click browser-based shell, AWS CLI, or RDP access without inbound port management Compliance \u0026amp; Monitoring: Full audit trails, session logging, command recording, and RDP session recordings Pricing Model: Free trial period (remainder of current billing + full next period), then pay-as-you-go with detailed usage-based pricing Takeaway: Just-in-time node access implements least-privilege access at scale, allowing organizations to eliminate long-term credentials while maintaining rapid incident response capabilities and comprehensive audit compliance.\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Understand Database fundamentals (SQL vs NoSQL). Learn JavaScript/Node.js basics (alternative for Lambda). Deep dive into S3 Storage classes. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch theory videos on JavaScript/Node.js - Practice: + Install Node.js on local machine or EC2 + Run a simple \u0026ldquo;Hello World\u0026rdquo; server 09/22/2025 09/22/2025 https://www.udemy.com/course/nodejs-the-complete-guide/ 3 - Watch videos of Module 4 (Storage) - Practice: + Create S3 Bucket + Upload/Download objects via Console 09/23/2025 09/23/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 4 - Watch videos and practice labs on S3 Versioning \u0026amp; Lifecycle - Practice: + Enable Bucket Versioning + Set up a lifecycle rule to move objects to Glacier 09/24/2025 09/24/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000057.awsstudygroup.com/vi/8-versioning/ 5 - Learn about Databases (DynamoDB) - Practice: + Create a simple DynamoDB table (Lab 60) + Add items to the table manually 09/25/2025 09/25/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000060.awsstudygroup.com/vi/ 6 - Review Networking Protocols (HTTP vs MQTT) - Practice: + Research MQTT protocol basics (essential for IoT) 09/26/2025 09/26/2025 https://mqtt.org/ Week 3 Achievements: Ran basic Node.js applications. Managed S3 buckets with Versioning and Lifecycle policies. Created and populated a NoSQL DynamoDB table. Understood the difference between HTTP and MQTT protocols. "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"During my internship, I participated in three major workshops from the AWS Cloud Mastery Series. Each event provided valuable technical training, practical insights, and networking opportunities with industry experts.\nEvent 1 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: Introduction to Foundation Models and Generative AI capabilities using Amazon Bedrock. The workshop covered Amazon SageMaker for traditional ML workflows, including data preparation, model training \u0026amp; tuning, and AutoML capabilities. Focused on effective prompting techniques (Chain of Thought), Retrieval-Augmented Generation (RAG) architecture for integrating internal knowledge bases, vector embeddings with Amazon Titan, and Bedrock Agents for multi-step workflows. Also covered safety mechanisms including AWS Guardrails and content filtering.\nOutcome: Gained comprehensive knowledge on reducing AI hallucinations and enhancing response accuracy through RAG and embeddings. Learned how to apply specific AWS AI services and prompting strategies to real-world projects. Understood the distinction between traditional ML and generative AI use cases. The event provided valuable insights into building AI agents and offered opportunities to network with skilled speakers and 348 industry professionals.\nEvent 2 Event Name: AWS Cloud Mastery Series #2: DevOps on AWS\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: Comprehensive full-day workshop on DevOps practices and CI/CD automation on AWS. Morning session covered DevOps culture with DORA metrics, AWS CodePipeline ecosystem (CodeCommit, CodeBuild, CodeDeploy), deployment strategies (Blue/Green, Canary, Rolling), and Infrastructure as Code with CloudFormation and AWS CDK. Afternoon session explored container services (Docker, Amazon ECR, ECS vs. EKS, AWS App Runner) and observability with CloudWatch, AWS X-Ray, and comprehensive monitoring best practices.\nOutcome: Gained clear DevOps roadmap and deeper understanding of application lifecycle management. Learned to implement CI/CD pipelines that minimize human errors through automation. Acquired practical skills in infrastructure-as-code templates ensuring consistency and reducing manual configuration. Understood monitoring and observability as critical components for deployment health and system stability. Networked with 316 DevOps professionals and gained insights into real-world case studies.\nEvent 3 Event Name: AWS Cloud Mastery Series #3: Well-Architected Security Pillar\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nDescription: Deep dive into cloud security through AWS Well-Architected Framework\u0026rsquo;s Security Pillar covering five key domains. Identity \u0026amp; Access Management: IAM architecture, service roles, temporary credentials, AWS IAM Identity Center, Service Control Policies, Permission Boundaries, and MFA options. Detection \u0026amp; Continuous Monitoring: CloudTrail, GuardDuty, Security Hub, multi-layer logging (VPC Flow Logs, ALB logs, S3 access logs), EventBridge automation, and Detection-as-Code. Infrastructure Protection: VPC segmentation, Security Groups vs. NACLs, AWS WAF, DDoS protection with Shield, Network Firewall, and container security. Data Protection: encryption at-rest (S3, RDS, EBS, DynamoDB), in-transit encryption, AWS KMS key management, Secrets Manager, and automated rotation patterns. Incident Response: IR lifecycle, incident playbooks, and response procedures.\nOutcome: Learned how to enforce least privilege principles effectively using SCPs and IAM boundaries. Gained practical skills in setting up automated alerting and cross-account event routing for real-time threat detection. Acquired valuable insights into incident response processes emphasizing prevention and automation to minimize downtime and security risks. Understood comprehensive logging and monitoring as foundational to modern security architecture. Connected with 355+ security professionals and industry experts to discuss emerging threats and best practices.\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/5-workshop/5.4-set-up-website/","title":"Set up website","tags":[],"description":"","content":"Set up Gitlab repository to deploy website and lambda code Go to this Gitlab repository: https://gitlab.com/tranngockhiet22062005/smart-office Download and deploy on your own repository Set up role for Gitlab to deploy website to S3 and deploy code to Lambda Function Create an IAM User with following attribute (view 5.2 if you forget) User name: smart-office-gitlab-ci Policy: { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;S3ListBucketAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:GetBucketLocation\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::fcj-smart-office-frontend-ACCOUNT_ID-dev\u0026#34;, \u0026#34;arn:aws:s3:::fcj-smart-office-lambda-ACCOUNT_ID-dev\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;S3ReadWriteAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:PutObject\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:PutObjectAcl\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::fcj-smart-office-frontend-ACCOUNT_ID-dev/*\u0026#34;, \u0026#34;arn:aws:s3:::fcj-smart-office-lambda-ACCOUNT_ID-dev/*\u0026#34; ] }, { \u0026#34;Sid\u0026#34;: \u0026#34;LambdaUpdateAccess\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;lambda:UpdateFunctionCode\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetFunctionConfiguration\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:lambda:*:*:function:SmartOffice-*\u0026#34; }, { \u0026#34;Sid\u0026#34;: \u0026#34;CloudFrontInvalidation\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudfront:CreateInvalidation\u0026#34;, \u0026#34;cloudfront:GetDistribution\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } You should replace ACCOUNT_ID with your AWS Account ID\nPolicy name: SmartOfficeGitlabAccess Navigate to the user In Summary, click Create access key For Use case, check Command Line Interface (CLI) Check I understand the above recommendation and want to proceed to create an access key. Click Next Click Create access key Click Download .csv file Config Gitlab variables Go to your Gitlab repository, click Setting \u0026gt; Variables Click Add variable For each Key and Value follow the table bellow to know where to get value Key Value AWS_ACCESS_KEY_ID In your smart-office-gitlab-ci_accessKeys.csv you have downloaded AWS_DEFAULT_REGION ap-southeast-1 (or anywhere you deploy the workshop) AWS_SECRET_ACCESS_KEY In your smart-office-gitlab-ci_accessKeys.csv you have downloaded CLOUDFRONT_DISTRIBUTION_ID CloudFront \u0026gt; Distributions \u0026gt; Your distribution ID S3_BUCKET_FRONTEND fcj-smart-office-frontend-ACCOUNT_ID-dev (replace ACCOUNT_ID with your AWS Account ID) S3_BUCKET_LAMBDA fcj-smart-office-lambda-ACCOUNT_ID-dev (replace ACCOUNT_ID with your AWS Account ID) STACK_NAME_AUTH SmartOffice-Authenticate-Lambda-Dev STACK_NAME_CRUD SmartOffice-Crud-Lambda-Dev STACK_NAME_IOT SmartOffice-IoT-Lambda-Dev STACK_NAME_READONLY SmartOffice-ReadOnly-Lambda-Dev VITE_API_BASE_URL API Gateway \u0026gt; SmartOffice-API-Gateway-Dev-Api \u0026gt; Stages \u0026gt; Invoke URL Push code into an init branch Merge branchs in this order: init -\u0026gt; dev, dev -\u0026gt; main "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Introduction to Serverless Computing (Lambda). Understanding Event-Driven Architecture. Deepen Networking knowledge (Private Subnets/NAT). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch videos on AWS Lambda - Practice: + Create a \u0026ldquo;Hello World\u0026rdquo; Lambda function using Python + Test the function in the console 09/29/2025 09/29/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i 3 - Watch videos on Lambda Triggers - Practice: + Configure S3 to trigger a Lambda function on upload 09/30/2025 09/30/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://docs.aws.amazon.com/lambda/latest/dg/lambda-invocation.html 4 - Watch videos on API Gateway - Practice: + Create a simple HTTP API + Integrate API Gateway with Lambda 10/01/2025 10/01/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000055.awsstudygroup.com/vi/3-create-single-page-app/3.3-create-api-with-api-gateway/ 5 - Review VPC Networking - Practice: + Create a Private Subnet (Lab 3 revisit) + Launch an instance in Private Subnet 10/02/2025 10/02/2025 https://000003.awsstudygroup.com/ 6 - Study Event-Driven concepts - Practice: + Draw a diagram of how an IoT sensor might trigger a Lambda function 10/03/2025 10/03/2025 https://aws.amazon.com/event-driven-architecture/ Week 4 Achievements: Created and tested Lambda functions with Python. Triggered Lambda automatically via S3 events. Exposed a Lambda function via API Gateway. Solidified VPC networking concepts regarding private subnets. "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/5-workshop/5.5-event-bridge/","title":"EventBridge and Lambda Setup","tags":[],"description":"","content":"Overview This section will guide you through setting up Amazon EventBridge and Lambda to route and react to events happening in DynamoDB. For SNS setup (used to send alerts), please refer to section 5.6 - SNS Setup.\nCreate AutomationSetup (Lambda + rules) Create a Lambda function (AutomationSetup) whose job is to read automation configuration from DynamoDB and define two EventBridge rules: one to turn automation ON and another to turn it OFF. import boto3 import json import os from boto3.dynamodb.types import TypeDeserializer deserializer = TypeDeserializer() events_client = boto3.client(\u0026#39;events\u0026#39;) HANDLER_ARN = os.environ.get(\u0026#39;HANDLER_LAMBDA_ARN\u0026#39;) def ddb_deserialize(image): d = {} for key in image: d[key] = deserializer.deserialize(image[key]) return d def time_to_cron(time_str): try: hour, minute = map(int, time_str.split(\u0026#39;:\u0026#39;)) utc_hour = hour - 7 if utc_hour \u0026lt; 0: utc_hour += 24 return f\u0026#34;cron({minute} {utc_hour} * * ? *)\u0026#34; except: return None # --- UPDATE 1: Add office_id parameter to function --- def create_or_update_schedule(room_id, office_id, time_str, action): rule_name = f\u0026#34;Room_{room_id}_Auto_{action}\u0026#34; cron_expr = time_to_cron(time_str) if not cron_expr: return print(f\u0026#34;Updating Rule: {rule_name} with Input\u0026#34;) events_client.put_rule( Name=rule_name, ScheduleExpression=cron_expr, State=\u0026#39;ENABLED\u0026#39;, Description=f\u0026#39;Auto {action} for {room_id} in {office_id}\u0026#39; ) # --- UPDATE 2: Add officeId to Input JSON --- target_input = json.dumps({ \u0026#34;roomId\u0026#34;: room_id, \u0026#34;officeId\u0026#34;: office_id, # \u0026lt;--- IMPORTANT: Include officeId \u0026#34;command\u0026#34;: action.upper(), \u0026#34;source\u0026#34;: \u0026#34;Scheduled_Event\u0026#34; }) events_client.put_targets( Rule=rule_name, Targets=[{ \u0026#39;Id\u0026#39;: \u0026#39;1\u0026#39;, \u0026#39;Arn\u0026#39;: HANDLER_ARN, \u0026#39;Input\u0026#39;: target_input }] ) def lambda_handler(event, context): print(\u0026#34;Raw Event:\u0026#34;, json.dumps(event)) if \u0026#39;Records\u0026#39; in event: for record in event[\u0026#39;Records\u0026#39;]: if record[\u0026#39;eventName\u0026#39;] in [\u0026#39;INSERT\u0026#39;, \u0026#39;MODIFY\u0026#39;]: raw_image = record[\u0026#39;dynamodb\u0026#39;][\u0026#39;NewImage\u0026#39;] data = ddb_deserialize(raw_image) room_id = data.get(\u0026#39;roomId\u0026#39;) # --- UPDATE 3: Get officeId from DynamoDB --- office_id = data.get(\u0026#39;officeId\u0026#39;) auto_control = data.get(\u0026#39;autoControl\u0026#39;) auto_on = data.get(\u0026#39;autoOnTime\u0026#39;) auto_off = data.get(\u0026#39;autoOffTime\u0026#39;) if auto_control == \u0026#34;ON\u0026#34; and room_id: # Pass office_id to schedule creation function if auto_on: create_or_update_schedule(room_id, office_id, auto_on, \u0026#39;ON\u0026#39;) if auto_off: create_or_update_schedule(room_id, office_id, auto_off, \u0026#39;OFF\u0026#39;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;Processed\u0026#39;} Go to Configuration \u0026ndash;\u0026gt; Trigger to configure the Lambda triggers — this will be used to invoke the lambda whenever there is any new stream from DynamoDB Select Add Trigger, Select DynamoDB and choose the table that contains rooms\u0026rsquo;s config. In Configuration \u0026ndash;\u0026gt; Environment Variable, add this key-value pair (replace accordingly with your personal information) Select Configuration \u0026ndash;\u0026gt;Role name, and make sure you have those 3 policies: AutomationSetup_RuleExecuiton for creating rules in EventBridge AWSLambdaBasicExecutionRole for basic lambda execution permissions AWSLambdaDynamoDBExecutionRole for interacting with DynamoDB. { \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;events:DeleteRule\u0026#34;, \u0026#34;events:PutTargets\u0026#34;, \u0026#34;events:EnableRule\u0026#34;, \u0026#34;events:PutRule\u0026#34;, \u0026#34;events:DisableRule\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Only for AutomationSetup_RuleExecuiton, select Add permissions \u0026ndash;\u0026gt; Create inline policy\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;arn:aws:logs:ap-southeast-1:261899902491:*\u0026#34; }, { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:logs:ap-southeast-1:261899902491:log-group:/aws/lambda/AutomationSetup:*\u0026#34; ] } ] } AWSLambdaBasicExecutionRole\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;dynamodb:DescribeStream\u0026#34;, \u0026#34;dynamodb:GetRecords\u0026#34;, \u0026#34;dynamodb:GetShardIterator\u0026#34;, \u0026#34;dynamodb:ListStreams\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:CreateLogStream\u0026#34;, \u0026#34;logs:PutLogEvents\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } AWSLambdaDynamoDBExecutionRole\nThese two rules correspond to the ON and OFF automation behaviours.\nCreate AutomationHandler (Lambda to forward events to AWS IoT Core) Create the AutomationHandler Lambda to receives events from EventBridge and forwards them to AWS IoT Core. import boto3 import json import logging logger = logging.getLogger() logger.setLevel(logging.INFO) iot_client = boto3.client(\u0026#39;iot-data\u0026#39;, region_name=\u0026#39;ap-southeast-1\u0026#39;) def lambda_handler(event, context): \u0026#34;\u0026#34;\u0026#34; Input from EventBridge: {\u0026#34;roomId\u0026#34;: \u0026#34;test2\u0026#34;, \u0026#34;officeId\u0026#34;: \u0026#34;...\u0026#34;, \u0026#34;command\u0026#34;: \u0026#34;ON\u0026#34;, ...} \u0026#34;\u0026#34;\u0026#34; # Log entire event to verify payload logger.info(f\u0026#34;Executing Automation: {json.dumps(event)}\u0026#34;) # 1. Extract data from Event room_id = event.get(\u0026#39;roomId\u0026#39;) command = event.get(\u0026#39;command\u0026#39;) # ON / OFF office_id = event.get(\u0026#39;officeId\u0026#39;) # 2. Validate input data if not room_id or not command: logger.error(\u0026#34;Missing roomId or command\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: \u0026#39;Missing roomId or command\u0026#39;} if not office_id: logger.error(\u0026#34;Missing officeId\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 400, \u0026#39;body\u0026#39;: \u0026#39;Missing officeId\u0026#39;} # 3. Create Topic and Payload topic = f\u0026#34;office/{office_id}/room/{room_id}/config\u0026#34; payload = { \u0026#34;command\u0026#34;: \u0026#34;SET_STATE\u0026#34;, \u0026#34;value\u0026#34;: command, \u0026#34;triggeredBy\u0026#34;: \u0026#34;Schedule\u0026#34; } # 4. Send command to IoT Core try: # This line must be aligned with lines above response = iot_client.publish( topic=topic, qos=1, payload=json.dumps(payload) ) logger.info(f\u0026#34;SUCCESS: Sent {command} to {topic}\u0026#34;) return {\u0026#39;statusCode\u0026#39;: 200, \u0026#39;body\u0026#39;: \u0026#39;Command sent\u0026#39;} except Exception as e: logger.error(f\u0026#34;IoT Publish Error: {e}\u0026#34;) # Raise error for EventBridge to know it failed (trigger Retry/DLQ) raise e Go to Configuration \u0026ndash;\u0026gt; Permissions and add resource-based policy. (To allow EventBridge to access this lambda function) Add the 2 rules created by AutomationSetup as a trigger for this Lambda. Replace REGION, ACCOUNT, ARNs and resource names with values from your account before running.\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Introduction to AWS IoT Core concepts. Introduction to SNS (Simple Notification Service). Final preparation before project kickoff. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Watch videos on SNS \u0026amp; SQS - Practice: + Create an SNS Topic + Subscribe an email address to the topic 10/06/2025 10/06/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://000077.awsstudygroup.com/1-introduce/ 3 - Learn about AWS IoT Core - Practice: + Read AWS IoT Core documentation regarding \u0026ldquo;Things\u0026rdquo; and \u0026ldquo;Shadows\u0026rdquo; 10/07/2025 10/07/2025 https://docs.aws.amazon.com/iot/latest/developerguide/what-is-aws-iot.html 4 - Watch and read theory on EventBridge - Practice: + Create a simple Scheduled Rule (Cron) in EventBridge + Target an SNS topic 10/08/2025 10/08/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://docs.aws.amazon.com/eventbridge/ 5 - Project Prep: + Review \u0026ldquo;Smart Office IoT\u0026rdquo; requirements + Meet with team to assign roles finalized 10/09/2025 10/09/2025 6 - Review: + Review JSON, IAM Policies, and Python syntax one last time 10/10/2025 10/10/2025 https://www.w3schools.com/python/ Week 5 Achievements: Configured SNS topics and email subscriptions. Created scheduled events using EventBridge. Gained theoretical understanding of IoT Core. Prepared fully for Project Kickoff. "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Smart Office Management System Workshop Overview Smart Office Management System provides a real-time environmental monitoring and management solution for offices, built entirely on AWS Serverless architecture to optimize costs and scalability.\nIn this lab, you will learn how to deploy, configure, and test a full-stack IoT system, allowing sensor devices to transmit data to the cloud and enabling administrators to control devices via a Web Dashboard.\nYou will work with two main architectural models to operate the Smart Office system:\nServerless Architecture - Uses AWS Lambda, API Gateway, and DynamoDB to handle logic and data storage. This model allows code to run in response to requests without managing servers. Event-Driven Architecture - Uses AWS IoT Core, EventBridge, and SNS. The system operates based on events, where data from sensors or user actions trigger automation workflows and send alert notifications. Content Workshop overview Prerequisite Run CloudFormation Stack Set up website Event Bridge SNS Test website IoT connection "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AMAZON WEB SERVICES VIETNAM CO .LTD from 06/08/2025 to 12/24/2025, I have learned so many things in the cloud domain and I felt grateful for all the time I spent in this program.\nMy team and I started Smart Office Project. From this project, I know more about the basic of many AWS Services such as Event Bridge, Lambda, SNS and IoT Core, as well as acquire a good grasp on the work flow of AWS architecture. Those knowledge will assist me strongly in my future endeavours.\nIn the project, I tried to complete my task dutifully, and also put myself in uncharted territory by learning to use more advanced features of some AWS Services, such ass Fleet Provisioning of AWS IoT Core.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ☐ ☐ ✅ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ☐ ✅ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ☐ ✅ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Be more disciplined Improve the understanding on front-end and the flow of the cloud architecture. Improve proactivity. "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/5-workshop/5.6-sns/","title":"SNS Setup","tags":[],"description":"","content":"Overview This section will guide you through setting up Amazon SNS (Simple Notification Service) to receive alerts from EventBridge when data anomalies are detected.\nCreate SNS Topic Go to Amazon SNS Console Select Topic and name your Topic DON\u0026rsquo;T TURN ON ENCRYPTION\nCreate SNS Subscription After creating the topic, create one or more subscriptions so alerts are delivered to recipients or endpoints (in this case, email).\nSteps:\nSelect Subscription in SNS Choose email protocol, select the topic you just created, and write down the email you want to test. Email endpoints require confirmation before they receive messages. Check your inbox for a confirmation email from AWS SNS and confirm the subscription.\nCreate a rule and attach it to the SNS topic Go to Amazon EventBridge Console Select Rules and create a new rule In step 2, we define the event pattern by choosing Custom pattern. Event patterns can match on source, detail-type.\nUse the following Json:\n{ \u0026#34;source\u0026#34;: [\u0026#34;com.smartoffice.iot\u0026#34;], \u0026#34;detail-type\u0026#34;: [\u0026#34;sensor.anomaly\u0026#34;] } In step 3, Select the target for the rule — choose the SNS topic you created earlier. After creation, EventBridge will forward matching events to the SNS topic which will deliver to its subscriptions.\nReplace email addresses and resource names with values from your account before running.\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: PROJECT START: Test IoT Set up AWS IoT Core Environment. Register IoT Things (Simulated Devices). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Project: Introduction to AWS IoT Core - Practice: + Log into AWS Console -\u0026gt; IoT Core + Create a \u0026ldquo;Thing Type\u0026rdquo; for Smart Sensors 10/13/2025 10/13/2025 https://docs.aws.amazon.com/iot/latest/developerguide/iot-thing-management.html 3 - Project: Register \u0026ldquo;Things\u0026rdquo; - Practice: + Create a single Thing (e.g., \u0026ldquo;OfficeTempSensor1\u0026rdquo;) + Download certificates and keys 10/14/2025 10/14/2025 https://docs.aws.amazon.com/iot/latest/developerguide/create-iot-resources.html 4 - Project: IoT Policies - Practice: + Create an IoT Policy allowing Connect, Publish, Subscribe + Attach Policy to the Certificate 10/15/2025 10/15/2025 https://docs.aws.amazon.com/iot/latest/developerguide/iot-policies.html 5 - Project: Device Connection Test - Practice: + Use MQTT Test Client in Console to simulate a device connecting 10/16/2025 10/16/2025 https://docs.aws.amazon.com/iot/latest/developerguide/view-mqtt-messages.html 6 - Project: Documentation - Practice: + Document the Thing ARN and Certificate IDs in project notes 10/17/2025 10/17/2025 Week 6 Achievements: Successfully test the following thing: Successfully initialized AWS IoT Core environment. Registered \u0026ldquo;Thing\u0026rdquo; representation for office sensors. Created and attached secure IoT Policies. Verified basic connectivity using MQTT Test Client. "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":"Overall Evaluation 1. Working Environment The working environment (Bitexco Financial Tower 26th Floor) is very spacious and cool, offering me and my peers a suitable environment to conduct team discussions.\n2. Support from Mentor Mentors in FCJ provide thorough explainations for each step we need to do in this internship program. They are hospitable, and ready to answer any question I have during the internship\u0026rsquo;s span.\n3. School program relevancy. As a major in Information Assurance, finding out about all the security aspects of the AWS platform have been a great delight. This program provide me clear guidance through Community-made labs to help me practices security measures.\n4.Skill Acquisition I have a stronger grasp to the cloud architecture and can think deeper about how to build a good and resilient application.\n5. Company Culture Everyone in this program is very supportive of each other, they do not shy away from giving constructive critisim and love to partake in various community events. Overall, this is a very healthy environment.\nAdditional Questions What did you find most satisfying during your internship? The working environment, and my supportive mentors and teammates. What do you think the company should improve for future interns? I am satisfied. If recommending to a friend, would you suggest they intern here? Why or why not? Yes, a very welcoming environment. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Create more exams. Would you like to continue this program in the future? It is unclear to me at the moment. Any other comments (free sharing):\nNo "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/5-workshop/5.7-test-website-iot-connection/","title":"Test website and IoT connection","tags":[],"description":"","content":"Download mock IoT device script main.py\nAdd data from your mock device ENDPOINT = \u0026#34;\u0026#34; CLIENT_ID = \u0026#34;\u0026#34; OFFICE_ID = \u0026#34;\u0026#34; ROOM_ID = \u0026#34;\u0026#34; PATH_TO_CERT = \u0026#34;\u0026#34; PATH_TO_KEY = \u0026#34;\u0026#34; PATH_TO_ROOT = \u0026#34;\u0026#34; Attribute Value ENDPOINT Website manager page CLIENT_ID ID of your office OFFICE_ID Name of your office ROOM_ID ID of your room PATH_TO_CERT Path to your device certificate download when your create room PATH_TO_KEY Path to your device private key download when your create room PATH_TO_ROOT Path to your Amazon root certifica\tdownload when your create room Link video demo https://www.youtube.com/watch?v=k45jHjkKhuc\n"},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Midterm Prep (Pillar 1): Master Secure Architectures (IAM, KMS, WAF, Shield, Secrets Manager). Midterm Prep (Pillar 2): Master Resilient Architectures (Multi-AZ, Auto Scaling, Route 53). Project Maintainence: Apply basic security concepts to the IoT setup (Conceptual). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Exam Prep (Security): + Deep dive into IAM (Policies, Roles, MFA) and SCP (Service Control Policies) + Study Encryption basics: KMS (Key Management) and TLS/ACM 10/20/2025 10/20/2025 https://docs.aws.amazon.com/wellarchitected/latest/security-pillar/welcome.html https://000033.awsstudygroup.com/vi/1-introduce/ 3 - Exam Prep (Network Security): + Compare Security Groups vs NACLs (Stateful vs Stateless) + Review Edge Security: AWS WAF, Shield, and GuardDuty 10/21/2025 10/21/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://aws.amazon.com/waf/ https://tutorialsdojo.com/security-group-vs-nacl/ 4 - Exam Prep (Resilience): + Study High Availability (HA) vs Disaster Recovery (DR) + Review Auto Scaling Groups (Launch Templates) and Load Balancing (ALB/NLB) 10/22/2025 10/22/2025 https://docs.aws.amazon.com/wellarchitected/latest/reliability-pillar/welcome.html https://viblo.asia/p/auto-scaling-group-tren-aws-EbNVQrxB4vR 5 - Exam Prep (Networking Reliability): + Deep dive into Route 53 (Routing policies: Simple, Weighted, Failover) + Study Backup \u0026amp; Restore strategies (RTO/RPO concepts) 10/23/2025 10/23/2025 https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/Welcome.html https://aws.amazon.com/blogs/architecture/disaster-recovery-dr-architecture-on-aws-part-ii-backup-and-restore-with-rapid-recovery/ 6 - Practice Exam: + Take a practice quiz focused on \u0026ldquo;Security\u0026rdquo; and \u0026ldquo;Reliability\u0026rdquo; + Review: Re-read notes on Secrets Manager vs Parameter Store 10/24/2025 10/24/2025 Week 7 Achievements: Mastered the difference between Stateful (SG) and Stateless (NACL) firewalls. Understood how to architect for failure using Multi-AZ and Auto Scaling. Gained familiarity with advanced security services (GuardDuty, WAF, KMS). Reviewed critical Disaster Recovery strategies. "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Midterm Prep (Pillar 3): High-Performing Architectures (Caching, Storage, Global Accelerator). Midterm Prep (Pillar 4): Cost-Optimized Architectures (Budgets, Tiering). Exam Day: Successfully complete the Midterm Exam (31/10). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Exam Prep (Performance): + Review Compute Scaling: EC2, Lambda, Fargate + Study Network Optimization: CloudFront (Caching) vs Global Accelerator 10/27/2025 10/27/2025 https://docs.aws.amazon.com/wellarchitected/latest/performance-efficiency-pillar/welcome.html https://docs.aws.amazon.com/global-accelerator/latest/dg/introduction-how-it-works.html 3 - Exam Prep (Storage \u0026amp; DB): + Compare Storage types: S3 (Classes), EFS, EBS + Review RDS basics (Read Replicas vs Multi-AZ) 10/28/2025 10/28/2025 https://www.youtube.com/playlist?list=PLahN4TLWtox2a3vElknwzU_urND8hLn1i https://renovacloud.com/en/ebs-vs-efs-which-storage-system-is-right-for-you/ 4 - Exam Prep (Cost Optimization): + Study Cost Explorer, Budgets, and Savings Plans + Review S3 Lifecycle Policies and Storage Tiering for cost savings 10/29/2025 10/29/2025 https://docs.aws.amazon.com/wellarchitected/latest/cost-optimization-pillar/welcome.html https://aws.amazon.com/aws-cost-management/ https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html 5 - Final Review: + Well-Architected Framework: Review all 6 pillars quickly + Flashcards: Review Port Numbers, Service Limits, and IAM JSON syntax 10/30/2025 10/30/2025 https://aws.amazon.com/architecture/well-architected/ 6 - MIDTERM EXAM DAY (31/10): + Task: Take the Midterm Exam covering Secure, Resilient, High-Performing, and Cost-Optimized Architectures 10/31/2025 10/31/2025 Week 8 Achievements: Deepened knowledge of Storage classes (S3/EBS/EFS) and their specific use cases. Learned how to optimize costs using Lifecycle policies and Savings Plans. Successfully completed the Midterm Exam. Consolidated knowledge of the AWS Well-Architected Framework. "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Implement MQTT Messaging for the Project. Define Topic Hierarchy. Simulate Data Publication. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Project: Define Topic Structure - Practice: + Finalize topics (e.g., office/room1/temp) 11/03/2025 11/03/2025 https://docs.aws.amazon.com/iot/latest/developerguide/topics.html 3 - Project: Python MQTT Script - Practice: + Write Python script using AWSIoTPythonSDK + Configure script with Certs from Week 6 11/04/2025 11/04/2025 https://github.com/aws/aws-iot-device-sdk-python 4 - Project: Data Simulation - Practice: + Update script to publish random JSON data 11/05/2025 11/05/2025 https://www.youtube.com/watch?v=kYJj7K0fT4Y 5 - Project: Verify Data - Practice: + Use AWS Console MQTT Client to subscribe to office/* and watch data flow 11/06/2025 11/06/2025 https://docs.aws.amazon.com/iot/latest/developerguide/view-mqtt-messages.html 6 - Project: Threading - Practice: + Add Threading logic to Python script for multiple devices simulation 11/07/2025 11/07/2025 https://www.geeksforgeeks.org/python/multithreading-python-set-1/ Week 9 Achievements: Defined a logical MQTT topic hierarchy for the Smart Office. Created a Python script to simulate hardware sensors. Successfully published telemetry data to AWS IoT Core. "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Configure AWS IoT Rules Engine. Integrate IoT Core with EventBridge. Filter incoming data. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Project: IoT Rules - Practice: + Write SQL statement to select data: e.g SELECT * FROM 'office/+/temp' 11/10/2025 11/10/2025 https://docs.aws.amazon.com/iot/latest/developerguide/iot-sql-reference.html 3 - Project: Create Rule - Practice: + Create an IoT Rule in Console + Set Action: \u0026ldquo;Send a message to EventBridge\u0026rdquo; 11/11/2025 11/11/2025 https://docs.aws.amazon.com/iot/latest/developerguide/eventbridge-rule.html 4 - Project: EventBridge Setup - Practice: + Go to EventBridge, check for incoming events 11/12/2025 11/12/2025 https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-iot-event.html 5 - Project: Event Pattern - Practice: + Create an EventBridge Rule with a pattern to match High Temp (\u0026gt; 40C) 11/13/2025 11/13/2025 https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-event-patterns.html 6 - Project: Testing Integration - Practice: + Publish high temp data via Python script + Verify Rule metric count increases 11/14/2025 11/14/2025 https://docs.aws.amazon.com/eventbridge/latest/userguide/eb-monitoring.html Week 10 Achievements: Configured IoT SQL Rules to filter message traffic. Established a direct integration between IoT Core and EventBridge. Defined EventBridge patterns to detect \u0026ldquo;High Temperature\u0026rdquo; anomalies. "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Security Hardening. Resource Cleanup. Monitoring Dashboard. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Project: Debugging - Practice: + Review IoT Policies + Fixing bugs and in the project 11/17/2025 11/17/2025 3 - Project: Debugging - Practice: + Fixing bugs and in the project 11/18/2025 11/18/2025 https://docs.aws.amazon.com/res/latest/ug/res-troubleshooting-general.html 4 - Project: Add Scheduling - Practice: + Add and test logic to the script for handling ON/OFF schedule for devices 11/19/2025 11/19/2025 https://www.hivemq.com/blog/implementing-mqtt-in-python/ 5 - Project: Code Cleanup - Practice: + Comment Python script extensively + Perform thorough testing to tackle edge cases 11/20/2025 11/20/2025 6 - Project: Test Run - Practice: + Run inital first version of the system to confirm flow 11/21/2025 11/21/2025 Week 11 Achievements: Fixing bugs for the project flow (review IoT Policy and permissions). Test schedule logic. Refactored code. Validated system capability. "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Improve coursera skill Skill Application: Master file manipulation, permissions, and piping. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Course: Module 1 (Introduction to Linux) 11/24/2025 11/24/2025 https://www.coursera.org/learn/hands-on-introduction-to-linux-commands-and-shell-scripting 3 - Course: Module 2 (Introduction to Linux Commands) 11/25/2025 11/25/2025 https://www.coursera.org/learn/hands-on-introduction-to-linux-commands-and-shell-scripting 4 - Course: Module 3 (Introduction to Shell Scripting) 11/26/2025 11/26/2025 https://www.coursera.org/learn/hands-on-introduction-to-linux-commands-and-shell-scripting 5 - Course: Module 5 (Final Project and Final Exam) 11/27/2025 11/27/2025 https://www.coursera.org/learn/hands-on-introduction-to-linux-commands-and-shell-scripting Week 12 Achievements: Completed \u0026ldquo;Hands-on Introduction to Linux Commands and Shell Scripting\u0026rdquo; on Coursera. Have a strong grasp of Linux and Shell Scripting "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/1-worklog/1.13-week13/","title":"Week 13 Worklog","tags":[],"description":"","content":"Week 13 Objectives: Final Testing Make Workshop, Demo Video and discuss presentation Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Project: Further Flow Testing - Practice: + Further testing to make sure the Front End application properly display all data points 12/01/2025 12/01/2025 3 - Project: Schedule Testing - Practice: + Testing Front End to send ON/OFF schedule to the script. 12/02/2025 12/02/2025 4 - Project: Make Workshop 12/03/2025 12/03/2025 5 - Project: Make Workshop 12/04/2025 12/04/2025 6 - Project: Make Demo Video 12/05/2025 12/05/2025 7 - Project: Presentation Dicussion 12/06/2025 12/06/2025 Week 13 Achievements: Finish the project! "},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://khangvo05.github.io/vo-an-khang-fptse190618-internship-report/tags/","title":"Tags","tags":[],"description":"","content":""}]